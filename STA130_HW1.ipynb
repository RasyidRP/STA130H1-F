{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fb7cc12",
   "metadata": {},
   "source": [
    "## STA130 HW 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971883ec",
   "metadata": {},
   "source": [
    "Rasyid Rafi Pamuji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298f78d",
   "metadata": {},
   "source": [
    "Student Number: 1011270081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5538ef52",
   "metadata": {},
   "source": [
    "1. Using the dataset from the STA130 Instructions page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9a34d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "row_n           0\n",
       "id              1\n",
       "name            0\n",
       "gender          0\n",
       "species         0\n",
       "birthday        0\n",
       "personality     0\n",
       "song           11\n",
       "phrase          0\n",
       "full_id         0\n",
       "url             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02d559",
   "metadata": {},
   "source": [
    "2. Using ChatGPT, I created a new session with the prompt of asking how many columns and rows of data a \"pandas\" Dataframe has. The prompt I used was \"Hey ChatGPT, I have a dataset and I need help on how many columns and rows of data a \"pandas\" DataFrame has. This is the dataset:\n",
    "\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\". Specifiy which is which.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faa38e",
   "metadata": {},
   "source": [
    "Code provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f324f4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 391, Columns: 11\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv\"\n",
    "villagers_df = pd.read_csv(url)\n",
    "\n",
    "# Get the number of rows and columns\n",
    "rows, columns = villagers_df.shape\n",
    "print(f\"Rows: {rows}, Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b63591",
   "metadata": {},
   "source": [
    "From my own understanding of the general definitions, an \"observation\" is each unique datapoint being measured in each of the rows in the dataset. In my dataset, each row represents a unique \"villager\". The \"variables\" are each of the characteristics or features given to the \"observations\", in my case, the villagers, in each of the columns in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5bb0b4",
   "metadata": {},
   "source": [
    "3. When asking the Chatbot for summaries of the columns in the dataset using: df.describe() and df['column'].value_counts(), it creates these codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "639c8ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             row_n       id     name gender species birthday personality  \\\n",
      "count   391.000000      390      391    391     391      391         391   \n",
      "unique         NaN      390      391      2      35      361           8   \n",
      "top            NaN  admiral  Admiral   male     cat     1-27        lazy   \n",
      "freq           NaN        1        1    204      23        2          60   \n",
      "mean    239.902813      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "std     140.702672      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "min       2.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "25%     117.500000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "50%     240.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "75%     363.500000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "max     483.000000      NaN      NaN    NaN     NaN      NaN         NaN   \n",
      "\n",
      "                song   phrase           full_id  \\\n",
      "count            380      391               391   \n",
      "unique            92      388               391   \n",
      "top     K.K. Country  wee one  villager-admiral   \n",
      "freq              10        2                 1   \n",
      "mean             NaN      NaN               NaN   \n",
      "std              NaN      NaN               NaN   \n",
      "min              NaN      NaN               NaN   \n",
      "25%              NaN      NaN               NaN   \n",
      "50%              NaN      NaN               NaN   \n",
      "75%              NaN      NaN               NaN   \n",
      "max              NaN      NaN               NaN   \n",
      "\n",
      "                                                      url  \n",
      "count                                                 391  \n",
      "unique                                                391  \n",
      "top     https://villagerdb.com/images/villagers/thumb/...  \n",
      "freq                                                    1  \n",
      "mean                                                  NaN  \n",
      "std                                                   NaN  \n",
      "min                                                   NaN  \n",
      "25%                                                   NaN  \n",
      "50%                                                   NaN  \n",
      "75%                                                   NaN  \n",
      "max                                                   NaN  \n",
      "species\n",
      "cat          23\n",
      "rabbit       20\n",
      "frog         18\n",
      "squirrel     18\n",
      "duck         17\n",
      "dog          16\n",
      "cub          16\n",
      "pig          15\n",
      "bear         15\n",
      "mouse        15\n",
      "horse        15\n",
      "bird         13\n",
      "penguin      13\n",
      "sheep        13\n",
      "elephant     11\n",
      "wolf         11\n",
      "ostrich      10\n",
      "deer         10\n",
      "eagle         9\n",
      "gorilla       9\n",
      "chicken       9\n",
      "koala         9\n",
      "goat          8\n",
      "hamster       8\n",
      "kangaroo      8\n",
      "monkey        8\n",
      "anteater      7\n",
      "hippo         7\n",
      "tiger         7\n",
      "alligator     7\n",
      "lion          7\n",
      "bull          6\n",
      "rhino         6\n",
      "cow           4\n",
      "octopus       3\n",
      "Name: count, dtype: int64\n",
      "personality\n",
      "lazy      60\n",
      "normal    59\n",
      "cranky    55\n",
      "snooty    55\n",
      "jock      55\n",
      "peppy     49\n",
      "smug      34\n",
      "uchi      24\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# General summary for numeric and categorical data\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "# Count the number of villagers for each species\n",
    "print(df['species'].value_counts())\n",
    "\n",
    "# Count the number of villagers for each personality type\n",
    "print(df['personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6104a44",
   "metadata": {},
   "source": [
    "4. I can find out if my dataset has non-numeric variables and missing values in numeric variables by using this code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9fb3ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_n           int64\n",
      "id             object\n",
      "name           object\n",
      "gender         object\n",
      "species        object\n",
      "birthday       object\n",
      "personality    object\n",
      "song           object\n",
      "phrase         object\n",
      "full_id        object\n",
      "url            object\n",
      "dtype: object\n",
      "row_n    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the data types of all columns\n",
    "print(df.dtypes)\n",
    "\n",
    "# Check for missing values in numeric columns\n",
    "print(df.select_dtypes(include=['number']).isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898527c",
   "metadata": {},
   "source": [
    "Non-numeric variables contain data types like \"object\". My dataset has no numeric variables with missing values as row_n is the only column and it has 0 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d01d3b",
   "metadata": {},
   "source": [
    "Instead, I will use the \"titanic.csv\" for further analysis into the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e566242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "titanic_df = pd.read_csv(url)\n",
    "\n",
    "# Get the shape of the dataset\n",
    "shape = titanic_df.shape\n",
    "print(f\"Shape of the dataset (rows, columns): {shape}\")\n",
    "\n",
    "# Get the descriptive statistics for numeric columns\n",
    "describe = titanic_df.describe()\n",
    "print(\"\\nDescriptive statistics for numeric columns:\")\n",
    "print(describe)\n",
    "\n",
    "# Check the count column from df.describe()\n",
    "print(\"\\n'Count' of non-missing values for each numeric column:\")\n",
    "print(describe.loc['count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a61ea0",
   "metadata": {},
   "source": [
    "Based on the coding analysis:\n",
    "- df.shape gives the total number of rows and columns.\n",
    "- df.describe() provides statistics for numeric columns only.\n",
    "- describe.loc['count'] shows the number of non-missing values for each numeric column.\n",
    "\n",
    "- df.shape shows 15 columns, but df.describe() only analyzes the numeric columns (6 in this case).\n",
    "- The age column has 714 non-missing values, as shown by the count in df.describe(), meaning 177 values are missing (891 - 714 = 177)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2832dd",
   "metadata": {},
   "source": [
    "Therefore, the discrepancies explained are:\n",
    "- df.shape reports the total number of rows and columns, including all variables (numeric and non-numeric).\n",
    "- df.describe() reports only numeric columns and gives a count of non-missing values, so it might show fewer columns and a smaller count of rows due to missing data in some columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa17ca9",
   "metadata": {},
   "source": [
    "5. Attributes and Methods\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdcd5cb",
   "metadata": {},
   "source": [
    "Attributes are characteristics or properties that store information about the DataFrame itself and only return stored data from the Dataframe. Therefore, they do not need any parentheses as they simply return data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b668d412",
   "metadata": {},
   "source": [
    "Methods, however, are functions that perform operations / calculations associated with a Dataframe. They require parentheses as they execute code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89460e7",
   "metadata": {},
   "source": [
    "6. The definitions of each summary statistics of df.describe():"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554070a",
   "metadata": {},
   "source": [
    "- count produces the number of non-missing (non-NaN) values in each column.\n",
    "- mean produces the arithmetic mean (average) of the values in each column.\n",
    "- std (standard Deviation) produces the measurement of the spread or dispersion of the values around the mean.\n",
    "- min (minimum) produces the smallest value in the column.\n",
    "- 25% (first quartile) produces the value below which 25% of the data falls. It's the first quartile (Q1), providing information about the lower part of the data distribution.\n",
    "- 50% (median or second quartile) produces the middle value of the data (50% of the data is below this value). It's the median and is also the second quartile (Q2).\n",
    "- 75% (third quartile) produces the value below which 75% of the data falls. It's the third quartile (Q3), giving insight into the upper part of the data distribution.\n",
    "- max (maximum) produces the largest value in the column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af5d18",
   "metadata": {},
   "source": [
    "7.1 I am creating a machine learning model that uses the titanic.csv dataset to predict whether passengers survived the Titanic disaster. In this case, the 'age' column has missing values, but it is important as age is a key factor (children given priority for survival). As a result, the 'age' column shouldn't be removed but we also don't want to include rows with missing age values. Therefore, df.dropna() is preferred over del df['col']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27969d6",
   "metadata": {},
   "source": [
    "7.2 Working with the same dataset, the 'deck' column has too many missing values to be useful. Even when it's filled, it doesn't provide essential information. Therefore, it is best to just delete the whole column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc2d423",
   "metadata": {},
   "source": [
    "7.3 It's important to have del df['col'] first before df.dropna() when used together because it can be used to avoid unnecessary row removal. If I use df.dropna() first, it can also shrink my dataset because it would unintentionally remove rows that could have meaningful information. Overall, having del df['col'] first before df.dropna() would ensure that it preserves data, be efficient, and produce cleaner results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39325ddd",
   "metadata": {},
   "source": [
    "7.4 First, we need to identify which columns have missing values and assess their importance to the dataset. Next, remove any irrelevant data using del df['col']. Drop rows from important columns using df.dropna(). In this case, we will drop the deck column as it is mostly empty with missing values. Then, we drop rows that have missing important data such as age, embarked, and fare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48125a6",
   "metadata": {},
   "source": [
    "Before and after results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b72cb504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6e00d3",
   "metadata": {},
   "source": [
    "In the end, we retrieved as much relevant data as possible. This results in a clean dataset with no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a178937",
   "metadata": {},
   "source": [
    "8.1 Based on the Chatbot session, df.groupby(\"col1\") groups the DataFrame df by the values in col1. This results in a new object where rows are split into groups based on the values of \"col1\". [\"col2\"] selects the column col2 from the grouped DataFrame. The analysis that follows will only be applied to this column within each group defined by col1. .describe() generates a summary of descriptive statistics for col2 within each group of col1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7b9f91",
   "metadata": {},
   "source": [
    "An example given by the Chatbot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848af7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>261.0</td>\n",
       "      <td>27.915709</td>\n",
       "      <td>14.110146</td>\n",
       "      <td>0.75</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>453.0</td>\n",
       "      <td>30.726645</td>\n",
       "      <td>14.678201</td>\n",
       "      <td>0.42</td>\n",
       "      <td>21.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        count       mean        std   min   25%   50%   75%   max\n",
       "sex                                                              \n",
       "female  261.0  27.915709  14.110146  0.75  18.0  27.0  37.0  63.0\n",
       "male    453.0  30.726645  14.678201  0.42  21.0  29.0  39.0  80.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"sex\")[\"age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482a60c5",
   "metadata": {},
   "source": [
    "To demonstrate understanding, another example from the \"titanic\" dataset is by using the 'parch' and 'fare' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817575ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>678.0</td>\n",
       "      <td>25.586774</td>\n",
       "      <td>41.878786</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.85420</td>\n",
       "      <td>10.50000</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.0</td>\n",
       "      <td>46.778180</td>\n",
       "      <td>63.709430</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>17.21250</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.0</td>\n",
       "      <td>64.337604</td>\n",
       "      <td>65.993088</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>32.88125</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.951660</td>\n",
       "      <td>7.862611</td>\n",
       "      <td>18.7500</td>\n",
       "      <td>19.25830</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>34.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>84.968750</td>\n",
       "      <td>118.731099</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>26.19375</td>\n",
       "      <td>27.90000</td>\n",
       "      <td>86.6750</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>32.550000</td>\n",
       "      <td>4.101134</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>31.27500</td>\n",
       "      <td>31.27500</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>39.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>46.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>46.90000</td>\n",
       "      <td>46.90000</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>46.9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count       mean         std      min       25%       50%      75%  \\\n",
       "parch                                                                       \n",
       "0      678.0  25.586774   41.878786   0.0000   7.85420  10.50000  26.0000   \n",
       "1      118.0  46.778180   63.709430   7.2292  17.21250  26.00000  39.6875   \n",
       "2       80.0  64.337604   65.993088   7.7500  26.00000  32.88125  69.5500   \n",
       "3        5.0  25.951660    7.862611  18.7500  19.25830  23.00000  34.3750   \n",
       "4        4.0  84.968750  118.731099  21.0750  26.19375  27.90000  86.6750   \n",
       "5        5.0  32.550000    4.101134  29.1250  31.27500  31.27500  31.3875   \n",
       "6        1.0  46.900000         NaN  46.9000  46.90000  46.90000  46.9000   \n",
       "\n",
       "            max  \n",
       "parch            \n",
       "0      512.3292  \n",
       "1      512.3292  \n",
       "2      263.0000  \n",
       "3       34.3750  \n",
       "4      263.0000  \n",
       "5       39.6875  \n",
       "6       46.9000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"parch\")[\"fare\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38e171a",
   "metadata": {},
   "source": [
    "'parch' is the number of parents and children onboard. 0 is none. 1 is one parent/child and so on. 'fare' is the fare paid by each passenger. 'count' is the number of passengers in that group (e.g., there are 678 passengers with parch = 0). 'mean' is the average fare for that group (e.g., passengers with parch = 0 paid an average fare of $25.586774). 'std' is the standard deviation of the fares for that group, which shows how much the fare values vary. 'min' is the lowest fare paid by someone in that group. '25%' means that 25% of the passengers in that group paid less than this amount. '50%' means that 50% of the passengers in that group paid less than this amount. '75%' means that 75% of passengers in that group paid less than this amount. 'max' is the maximum fare paid by someone in that group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd86a54",
   "metadata": {},
   "source": [
    "8.2 df.describe() gives you an overall dataset summary, where 'count' reflects how many values exist per column.\n",
    "df.groupby(\"col1\")[\"col2\"].describe() is more focused on group-based analysis, with 'count' representing the size of each group. The main goal of the groupby() count is not just to exclude missing data, but to show how the data is distributed among groups, which is essential for making comparisons between categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3a753",
   "metadata": {},
   "source": [
    "8.3 Assessment of Chatbot vs. Google Performance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dacc56",
   "metadata": {},
   "source": [
    "Using code from the \"before and after\" results from no. 7, we can test out if Chatbot or Google has better results for fixing code errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f47cfd8",
   "metadata": {},
   "source": [
    "A. Forgetting to include import pandas as pd in my code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbd4bcc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Step 1: Check missing values before cleanup\u001b[39;00m\n\u001b[1;32m      6\u001b[0m missing_before \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e414698b",
   "metadata": {},
   "source": [
    "Chatbot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bc0c2c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a72f8b",
   "metadata": {},
   "source": [
    "Result: Fixes code instantly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a70082",
   "metadata": {},
   "source": [
    "Google response after searching \"NameError: name 'pd' is not defined\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4428dc5",
   "metadata": {},
   "source": [
    "Found solution in: https://stackoverflow.com/questions/40407090/nameerror-name-pd-is-not-defined. Took a bit of reading and some time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec81bd3a",
   "metadata": {},
   "source": [
    "B. Mistype \"titanic.csv\" as \"titanics.csv\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16d88c38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Check missing values before cleanup\u001b[39;00m\n\u001b[1;32m      8\u001b[0m missing_before \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902bb18",
   "metadata": {},
   "source": [
    "Chatbot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bbf33",
   "metadata": {},
   "source": [
    "Result: Found problem and fixes code instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee7e713",
   "metadata": {},
   "source": [
    "Google response after copy and pasting first paragraph of error code:\n",
    "\n",
    "Found correct link to dataset: https://github.com/mwaskom/seaborn-data/blob/master/titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880eb97e",
   "metadata": {},
   "source": [
    "Other typos in URL yields similar error codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "736d8096",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the Titanic dataset\u001b[39;00m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaburn-data/mastre/titanics.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Check missing values before cleanup\u001b[39;00m\n\u001b[1;32m      8\u001b[0m missing_before \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:718\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    715\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[1;32m    717\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[0;32m--> 718\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[1;32m    727\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:372\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# assuming storage_options is to be interpreted as headers\u001b[39;00m\n\u001b[1;32m    371\u001b[0m req_info \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mRequest(filepath_or_buffer, headers\u001b[38;5;241m=\u001b[39mstorage_options)\n\u001b[0;32m--> 372\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq_info\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m req:\n\u001b[1;32m    373\u001b[0m     content_encoding \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    375\u001b[0m         \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/io/common.py:274\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;03mLazy-import wrapper for stdlib urlopen, as that imports a big chunk of\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03mthe stdlib.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrequest\u001b[39;00m\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:525\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m processor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response\u001b[38;5;241m.\u001b[39mget(protocol, []):\n\u001b[1;32m    524\u001b[0m     meth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(processor, meth_name)\n\u001b[0;32m--> 525\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmeth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:634\u001b[0m, in \u001b[0;36mHTTPErrorProcessor.http_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# According to RFC 2616, \"2xx\" code indicates that the client's\u001b[39;00m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;66;03m# request was successfully received, understood, and accepted.\u001b[39;00m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m):\n\u001b[0;32m--> 634\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhdrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:563\u001b[0m, in \u001b[0;36mOpenerDirector.error\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_err:\n\u001b[1;32m    562\u001b[0m     args \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp_error_default\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m orig_args\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/urllib/request.py:643\u001b[0m, in \u001b[0;36mHTTPDefaultErrorHandler.http_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttp_error_default\u001b[39m(\u001b[38;5;28mself\u001b[39m, req, fp, code, msg, hdrs):\n\u001b[0;32m--> 643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(req\u001b[38;5;241m.\u001b[39mfull_url, code, msg, hdrs, fp)\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaburn-data/mastre/titanics.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f07526",
   "metadata": {},
   "source": [
    "C. Changing df to DF for every instance in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58bd9c60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Step 1: Check missing values before cleanup\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m missing_before \u001b[38;5;241m=\u001b[39m \u001b[43mDF\u001b[49m\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Step 2: Remove irrelevant columns with many missing values\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m DF[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeck\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DF' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = DF.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del DF['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "DF_clean = DF.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = DF_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = DF.shape\n",
    "shape_after = DF_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a1177",
   "metadata": {},
   "source": [
    "Chatbot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18541947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef325fc",
   "metadata": {},
   "source": [
    "Result: changed DF to df in every instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed52904",
   "metadata": {},
   "source": [
    "Google response after copy and pasting error code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac35e752",
   "metadata": {},
   "source": [
    "Couldn't find reasonable answer to my code. Only relevant answer was from: https://pandas.pydata.org/docs/user_guide/missing_data.html, but had to read a lot to find my answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb00d1a3",
   "metadata": {},
   "source": [
    "D. Code with missing parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d103c3a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4117513225.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 5\u001b[0;36m\u001b[0m\n\u001b[0;31m    df = pd.read_csv(url\u001b[0m\n\u001b[0m                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum(\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47152cf0",
   "metadata": {},
   "source": [
    "Chatbot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d6beb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)  # Added missing parenthesis\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()  # Added missing parenthesis\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8a965",
   "metadata": {},
   "source": [
    "Result: Fixed both of my missing parentheses, and not only one that is said in the error output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0221ff2f",
   "metadata": {},
   "source": [
    "Google response: Pretty easy to find. All I needed to do was fix missing parentheses. Link: https://stackoverflow.com/questions/74749656/why-am-i-getting-invalid-syntax-perhaps-you-forgot-a-comma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50813d29",
   "metadata": {},
   "source": [
    "E. Mistype one of the names of the chained functions with the code by changing dropna to dripna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f01a046c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'dripna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_177/1660139872.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Step 2: Remove irrelevant columns with many missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'deck'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Step 3: Drop rows with missing values in essential columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdf_clean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdripna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Step 4: Check missing values after cleanup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmissing_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6200\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6201\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6202\u001b[0m         ):\n\u001b[1;32m   6203\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6204\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'dripna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dripna()\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63453c16",
   "metadata": {},
   "source": [
    "Chatbot response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d52eab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleanup - Missing Values Per Column:\n",
      "survived         0\n",
      "pclass           0\n",
      "sex              0\n",
      "age            177\n",
      "sibsp            0\n",
      "parch            0\n",
      "fare             0\n",
      "embarked         2\n",
      "class            0\n",
      "who              0\n",
      "adult_male       0\n",
      "deck           688\n",
      "embark_town      2\n",
      "alive            0\n",
      "alone            0\n",
      "dtype: int64\n",
      "\n",
      "After Cleanup - Missing Values Per Column:\n",
      "survived       0\n",
      "pclass         0\n",
      "sex            0\n",
      "age            0\n",
      "sibsp          0\n",
      "parch          0\n",
      "fare           0\n",
      "embarked       0\n",
      "class          0\n",
      "who            0\n",
      "adult_male     0\n",
      "embark_town    0\n",
      "alive          0\n",
      "alone          0\n",
      "dtype: int64\n",
      "\n",
      "Shape of the dataset before cleanup: (891, 14)\n",
      "Shape of the dataset after cleanup: (712, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Step 1: Check missing values before cleanup\n",
    "missing_before = df.isnull().sum()\n",
    "\n",
    "# Step 2: Remove irrelevant columns with many missing values\n",
    "del df['deck']\n",
    "\n",
    "# Step 3: Drop rows with missing values in essential columns\n",
    "df_clean = df.dropna()  # Fixed typo here\n",
    "\n",
    "# Step 4: Check missing values after cleanup\n",
    "missing_after = df_clean.isnull().sum()\n",
    "\n",
    "# Step 5: Report the shape of the dataset before and after\n",
    "shape_before = df.shape\n",
    "shape_after = df_clean.shape\n",
    "\n",
    "print(\"Before Cleanup - Missing Values Per Column:\")\n",
    "print(missing_before)\n",
    "print(\"\\nAfter Cleanup - Missing Values Per Column:\")\n",
    "print(missing_after)\n",
    "\n",
    "print(f\"\\nShape of the dataset before cleanup: {shape_before}\")\n",
    "print(f\"Shape of the dataset after cleanup: {shape_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597b6a2",
   "metadata": {},
   "source": [
    "Result: Changed dripna back to dropna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1b9b51",
   "metadata": {},
   "source": [
    "Google response after searching \"AttributeError\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b5760c",
   "metadata": {},
   "source": [
    "Found web link that contains information on what I did wrong: https://www.geeksforgeeks.org/python-attributeerror/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05ba9b1",
   "metadata": {},
   "source": [
    "F. Using a column name that's not in my data for the groupby and column selection by capitalizing parch -> Parch and fare -> Fare in a separate code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36a62c9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Parch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mParch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:8869\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   8867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 8869\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8872\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8875\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8879\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1278\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1275\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1278\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_default\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1009\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1008\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1009\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Parch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.groupby(\"Parch\")[\"fare\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae0850f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Column not found: Fare'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFare\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/groupby/generic.py:1964\u001b[0m, in \u001b[0;36mDataFrameGroupBy.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(key) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;66;03m# if len == 1, then it becomes a SeriesGroupBy and this is actually\u001b[39;00m\n\u001b[1;32m   1959\u001b[0m     \u001b[38;5;66;03m# valid syntax, so don't raise\u001b[39;00m\n\u001b[1;32m   1960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot subset columns with a tuple with more than one element. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse a list instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1963\u001b[0m     )\n\u001b[0;32m-> 1964\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/base.py:244\u001b[0m, in \u001b[0;36mSelectionMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj:\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj[key]\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39mndim)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Column not found: Fare'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.groupby(\"parch\")[\"Fare\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691969e3",
   "metadata": {},
   "source": [
    "Chatbot responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8344df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>678.0</td>\n",
       "      <td>25.586774</td>\n",
       "      <td>41.878786</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.85420</td>\n",
       "      <td>10.50000</td>\n",
       "      <td>26.0000</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.0</td>\n",
       "      <td>46.778180</td>\n",
       "      <td>63.709430</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>17.21250</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>39.6875</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>80.0</td>\n",
       "      <td>64.337604</td>\n",
       "      <td>65.993088</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>32.88125</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>25.951660</td>\n",
       "      <td>7.862611</td>\n",
       "      <td>18.7500</td>\n",
       "      <td>19.25830</td>\n",
       "      <td>23.00000</td>\n",
       "      <td>34.3750</td>\n",
       "      <td>34.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>84.968750</td>\n",
       "      <td>118.731099</td>\n",
       "      <td>21.0750</td>\n",
       "      <td>26.19375</td>\n",
       "      <td>27.90000</td>\n",
       "      <td>86.6750</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>32.550000</td>\n",
       "      <td>4.101134</td>\n",
       "      <td>29.1250</td>\n",
       "      <td>31.27500</td>\n",
       "      <td>31.27500</td>\n",
       "      <td>31.3875</td>\n",
       "      <td>39.6875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>46.900000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>46.90000</td>\n",
       "      <td>46.90000</td>\n",
       "      <td>46.9000</td>\n",
       "      <td>46.9000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count       mean         std      min       25%       50%      75%  \\\n",
       "parch                                                                       \n",
       "0      678.0  25.586774   41.878786   0.0000   7.85420  10.50000  26.0000   \n",
       "1      118.0  46.778180   63.709430   7.2292  17.21250  26.00000  39.6875   \n",
       "2       80.0  64.337604   65.993088   7.7500  26.00000  32.88125  69.5500   \n",
       "3        5.0  25.951660    7.862611  18.7500  19.25830  23.00000  34.3750   \n",
       "4        4.0  84.968750  118.731099  21.0750  26.19375  27.90000  86.6750   \n",
       "5        5.0  32.550000    4.101134  29.1250  31.27500  31.27500  31.3875   \n",
       "6        1.0  46.900000         NaN  46.9000  46.90000  46.90000  46.9000   \n",
       "\n",
       "            max  \n",
       "parch            \n",
       "0      512.3292  \n",
       "1      512.3292  \n",
       "2      263.0000  \n",
       "3       34.3750  \n",
       "4      263.0000  \n",
       "5       39.6875  \n",
       "6       46.9000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Print the column names to verify\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Check the column names\n",
    "# If 'Parch' is not listed, then use the correct column name\n",
    "# For example, if 'parch' is the correct column name, use it in the groupby\n",
    "\n",
    "df.groupby(\"parch\")[\"fare\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9c34195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare',\n",
      "       'embarked', 'class', 'who', 'adult_male', 'deck', 'embark_town',\n",
      "       'alive', 'alone'],\n",
      "      dtype='object')\n",
      "       count       mean         std      min       25%       50%      75%  \\\n",
      "parch                                                                       \n",
      "0      678.0  25.586774   41.878786   0.0000   7.85420  10.50000  26.0000   \n",
      "1      118.0  46.778180   63.709430   7.2292  17.21250  26.00000  39.6875   \n",
      "2       80.0  64.337604   65.993088   7.7500  26.00000  32.88125  69.5500   \n",
      "3        5.0  25.951660    7.862611  18.7500  19.25830  23.00000  34.3750   \n",
      "4        4.0  84.968750  118.731099  21.0750  26.19375  27.90000  86.6750   \n",
      "5        5.0  32.550000    4.101134  29.1250  31.27500  31.27500  31.3875   \n",
      "6        1.0  46.900000         NaN  46.9000  46.90000  46.90000  46.9000   \n",
      "\n",
      "            max  \n",
      "parch            \n",
      "0      512.3292  \n",
      "1      512.3292  \n",
      "2      263.0000  \n",
      "3       34.3750  \n",
      "4      263.0000  \n",
      "5       39.6875  \n",
      "6       46.9000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Print column names to verify the correct names\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Use the correct column names for grouping and describing\n",
    "df_grouped = df.groupby(\"parch\")[\"fare\"].describe()\n",
    "\n",
    "print(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99cea42",
   "metadata": {},
   "source": [
    "Results: Both errors got returned with correct code fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b096ab08",
   "metadata": {},
   "source": [
    "Google responses after searching about both errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d253d8",
   "metadata": {},
   "source": [
    "Searching \"KeyError\" yields relevant web link: https://stackoverflow.com/questions/10116518/im-getting-key-error-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26098070",
   "metadata": {},
   "source": [
    "Searching \"KeyError: 'Column not found: Fare'\" yields relevant web link: https://stackoverflow.com/questions/61987268/key-error-column-not-found-but-its-there"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36466b5",
   "metadata": {},
   "source": [
    "G. Forgetting to put string in quotes to column names for the groupby and column selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40c71b72",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 7\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[43mparch\u001b[49m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfare\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parch' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.groupby(parch)[\"fare\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86ff7f31",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fare' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(url)\n\u001b[0;32m----> 7\u001b[0m df\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparch\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[43mfare\u001b[49m]\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fare' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "df.groupby(\"parch\")[fare].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ddba44",
   "metadata": {},
   "source": [
    "Chatbot responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99db84d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count       mean         std      min       25%       50%      75%  \\\n",
      "parch                                                                       \n",
      "0      678.0  25.586774   41.878786   0.0000   7.85420  10.50000  26.0000   \n",
      "1      118.0  46.778180   63.709430   7.2292  17.21250  26.00000  39.6875   \n",
      "2       80.0  64.337604   65.993088   7.7500  26.00000  32.88125  69.5500   \n",
      "3        5.0  25.951660    7.862611  18.7500  19.25830  23.00000  34.3750   \n",
      "4        4.0  84.968750  118.731099  21.0750  26.19375  27.90000  86.6750   \n",
      "5        5.0  32.550000    4.101134  29.1250  31.27500  31.27500  31.3875   \n",
      "6        1.0  46.900000         NaN  46.9000  46.90000  46.90000  46.9000   \n",
      "\n",
      "            max  \n",
      "parch            \n",
      "0      512.3292  \n",
      "1      512.3292  \n",
      "2      263.0000  \n",
      "3       34.3750  \n",
      "4      263.0000  \n",
      "5       39.6875  \n",
      "6       46.9000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Corrected code with 'parch' in quotes\n",
    "df_grouped = df.groupby(\"parch\")[\"fare\"].describe()\n",
    "\n",
    "print(df_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66963bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       count       mean         std      min       25%       50%      75%  \\\n",
      "parch                                                                       \n",
      "0      678.0  25.586774   41.878786   0.0000   7.85420  10.50000  26.0000   \n",
      "1      118.0  46.778180   63.709430   7.2292  17.21250  26.00000  39.6875   \n",
      "2       80.0  64.337604   65.993088   7.7500  26.00000  32.88125  69.5500   \n",
      "3        5.0  25.951660    7.862611  18.7500  19.25830  23.00000  34.3750   \n",
      "4        4.0  84.968750  118.731099  21.0750  26.19375  27.90000  86.6750   \n",
      "5        5.0  32.550000    4.101134  29.1250  31.27500  31.27500  31.3875   \n",
      "6        1.0  46.900000         NaN  46.9000  46.90000  46.90000  46.9000   \n",
      "\n",
      "            max  \n",
      "parch            \n",
      "0      512.3292  \n",
      "1      512.3292  \n",
      "2      263.0000  \n",
      "3       34.3750  \n",
      "4      263.0000  \n",
      "5       39.6875  \n",
      "6       46.9000  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Corrected code with 'fare' in quotes\n",
    "df_grouped = df.groupby(\"parch\")[\"fare\"].describe()\n",
    "\n",
    "print(df_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48e3f8",
   "metadata": {},
   "source": [
    "Results: Both errors got returned with correct code fixes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041553f3",
   "metadata": {},
   "source": [
    "Google responses after searching about both errors:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d56d5b0",
   "metadata": {},
   "source": [
    "Searching \"Python NameError\" yields relevant web link: https://stackoverflow.com/questions/14804084/python-nameerror-name-is-not-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd96e6f2",
   "metadata": {},
   "source": [
    "Summary: Based on the assessment experiment, ChatGPT can correct code almost instantly while Googling takes a lot of time to find the solution to the code errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ead88",
   "metadata": {},
   "source": [
    "9. Yes, I reviewed the course wiki-textbook and interacted with a Chatbot for this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a385c95",
   "metadata": {},
   "source": [
    "Chatbot Log Histories:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c9703",
   "metadata": {},
   "source": [
    "A. Error code exercise chat summary:\n",
    "\n",
    "link: https://chatgpt.com/share/66e386ad-3800-8004-a9c0-e6ccb8ae34fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fbf92b",
   "metadata": {},
   "source": [
    "Certainly! Heres a summary of our session:\n",
    "\n",
    "1. **Code Issues**:\n",
    "   - We explored various errors in your code related to loading and cleaning the Titanic dataset. These included `NameError` and `KeyError` due to incorrect column names and syntax issues.\n",
    "   - You encountered errors like `HTTPError` when trying to access data from an incorrect URL and `KeyError` when column names used in operations were not found in the DataFrame.\n",
    "\n",
    "2. **Correcting Errors**:\n",
    "   - We corrected issues related to invalid column names and syntax errors. For instance, ensuring column names like `'fare'` are used with correct capitalization and enclosing column names in quotes.\n",
    "\n",
    "3. **Best Practices**:\n",
    "   - Always check the actual column names in the DataFrame using `df.columns` to avoid referencing non-existent columns.\n",
    "   - Use quotes around column names when indexing or performing operations on DataFrame columns.\n",
    "\n",
    "If you have any more questions or need further assistance, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe349ec",
   "metadata": {},
   "source": [
    "B. No. 8 Chatbot Session:\n",
    "\n",
    "link: https://chatgpt.com/share/66e3867f-dea4-8004-8dd6-ec05e4f30031"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c11b72",
   "metadata": {},
   "source": [
    "Heres a summary of our session:\n",
    "\n",
    "1. **Titanic Dataset Analysis**:\n",
    "   - **`df.groupby(\"col1\")[\"col2\"].describe()`**: Groups the DataFrame by `col1` and generates descriptive statistics for `col2` within each group. The statistics include count, mean, standard deviation, min, quartiles, and max.\n",
    "   - **`df.groupby(\"parch\")[\"fare\"].describe()`**: Groups the Titanic dataset by `parch` (number of parents/children aboard) and provides descriptive statistics for the `fare` column for each group. This includes count, mean, standard deviation, min, 25th percentile, median, 75th percentile, and max of fares for each group defined by `parch`.\n",
    "\n",
    "2. **Understanding Output**:\n",
    "   - Each group in `parch` will have its own set of statistics summarizing the fare distribution. This helps analyze how the fare varies with the number of parents/children aboard.\n",
    "\n",
    "If you have any more questions or need further clarification, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8981b4d6",
   "metadata": {},
   "source": [
    "C. Overall HW1 Chatbot Session:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cb8f51",
   "metadata": {},
   "source": [
    "link: https://chatgpt.com/share/66e3910a-3808-8004-bf48-382cc4845e74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b25efe",
   "metadata": {},
   "source": [
    "In this session, we covered several key concepts related to data analysis with pandas, particularly using the Titanic dataset. Here's a summary:\n",
    "\n",
    "1. **Counting Rows and Columns in a DataFrame**: We explored how to check the shape of a dataset using `df.shape` to get the number of rows and columns.\n",
    "\n",
    "2. **Understanding Observations and Variables**: We discussed the general definitions of observations (rows) and variables (columns) in a dataset.\n",
    "\n",
    "3. **Summarizing Data**: You learned how to generate summary statistics with `df.describe()` and count unique values with `df['column'].value_counts()`.\n",
    "\n",
    "4. **Handling Missing Data**: We examined the presence of missing values in the Titanic dataset and discussed methods like `df.dropna()` to remove missing data, as well as `del df['col']` to delete entire columns that are irrelevant or contain too many missing values.\n",
    "\n",
    "5. **Comparison of `df.describe()` and `groupby()`**: We explored the differences between the `count` values in `df.describe()` and those in `df.groupby(\"col1\")[\"col2\"].describe()`. The focus was on how `df.describe()` handles missing values globally while `groupby()` focuses on group-wise analysis.\n",
    "\n",
    "6. **Efficient Data Cleaning**: We used a combination of `del df['col']` and `df.dropna()` to efficiently clean the dataset, explaining when one method would be preferred over the other.\n",
    "\n",
    "7. **Use Cases**: We discussed scenarios where removing irrelevant columns before handling missing values is important to avoid unnecessary data loss and improve efficiency.\n",
    "\n",
    "Would you like to continue or revisit any specific part of our discussion?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2c501",
   "metadata": {},
   "source": [
    "End of HW1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd5bfa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
