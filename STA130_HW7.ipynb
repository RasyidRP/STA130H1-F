{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbfaf08",
   "metadata": {},
   "source": [
    "# STA130 HW 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868bc456",
   "metadata": {},
   "source": [
    "Rasyid Rafi Pamuji\n",
    "\n",
    "Student Number: 1011270081"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e7c0b",
   "metadata": {},
   "source": [
    "## 1. Explain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647b500d",
   "metadata": {},
   "source": [
    "### 1.1 The difference between Simple Linear Regression and Multiple Linear Regression; and the benefit the latter provides over the former"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792bae2a",
   "metadata": {},
   "source": [
    "Simple Linear Regression uses one predictor variable to model a linear relationship with an outcome variable. The linear form is:\n",
    "\n",
    "outcome = $\\beta_0$ + $\\beta_1$ predictor + $\\epsilon_i$\n",
    "\n",
    "where:\n",
    "\n",
    "outcome = dependent variable\n",
    "\n",
    "$\\beta_0$ = intercept coefficient\n",
    "\n",
    "$\\beta_1$ = slope coefficient\n",
    "\n",
    "predictor = independent variable\n",
    "\n",
    "$\\epsilon_i$ = error term\n",
    "\n",
    "Multiple Linear Regression uses multiple predictor variables to show how multiple factors influence the outcome variable. The linear form is:\n",
    "\n",
    "outcome = $\\beta_0$ + $\\beta_1$ predictor$_1$ + $\\beta_2$ predictor$_2$ + ... + $\\epsilon_i$\n",
    "\n",
    "where:\n",
    "\n",
    "outcome = dependent variable\n",
    "\n",
    "$\\beta_0$ = intercept coefficient\n",
    "\n",
    "$\\beta_1$, $\\beta_2$, $\\beta_n$ = slope coefficients for each predictor variable\n",
    "\n",
    "predictor$_1$, predictor$_2$, predictor$_n$ = independent variables\n",
    "\n",
    "$\\epsilon_i$ = error term\n",
    "\n",
    "Benefits of Multiple Linear Regression:\n",
    "\n",
    "The ability to account for multiple predictors on the outcome variable. The advantages are increased accuracy, control for confounding variables (isolate the relationship between a specific predictor and the outcome, holding other factors constant), modeling interactions, and greater flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd1088",
   "metadata": {},
   "source": [
    "### 1.2 The difference between using a continuous variable and an indicator variable in Simple Linear Regression; and these two linear forms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f9155",
   "metadata": {},
   "source": [
    "Continuous variables are used to model a linear relationship between the outcome variable and the predictor variable. Indicator variables, on the other hand, are used to represent categorical data in a regression model.\n",
    "\n",
    "The linear form of a simple linear regression model using a continuous variable is:\n",
    "\n",
    "outcome = $\\beta_0$ + $\\beta_1$ predictor\n",
    "\n",
    "where:\n",
    "\n",
    "outcome = dependent variable\n",
    "\n",
    "$\\beta_0$ = intercept coefficient\n",
    "\n",
    "$\\beta_1$ = slope coefficient\n",
    "\n",
    "predictor = continuous independent variable\n",
    "\n",
    "The linear form of a simple linear regression model using an indicator variable is:\n",
    "\n",
    "outcome = $\\beta_0$ + $\\beta_1$ 1(predictor)\n",
    "\n",
    "where:\n",
    "\n",
    "outcome = dependent variable\n",
    "\n",
    "$\\beta_0$ = intercept coefficient\n",
    "\n",
    "$\\beta_1$ = contrast coefficient, capturing the difference between the two groups represented by the indicator variable\n",
    "\n",
    "1(predictor) = indicator variable, taking the value 1 for one group, and 0 for the other\n",
    "\n",
    "The key difference between these two linear forms is the interpretation of the slope coefficient.\n",
    "\n",
    "In the continuous variable model, $\\beta_1$ represents the average change in the outcome variable for a one-unit increase in the predictor variable.\n",
    "\n",
    "In the indicator variable model, $\\beta_1$ represents the difference in the average outcome between the two groups represented by the indicator variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab421ea",
   "metadata": {},
   "source": [
    "### 1.3 The change that happens in the behavior of the model (i.e., the expected nature of the data it models) when a single indicator variable is introduced alongside a continuous variable to create a Multiple Linear Regression; and these two linear forms (i.e., the Simple Linear Regression versus the Multiple Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee64fa79",
   "metadata": {},
   "source": [
    "When a single indicator variable is introduced alongside a continuous variable in a multiple linear regression model, the model's behavior shifts from representing a single straight line to representing two parallel lines.\n",
    "\n",
    "This change reflects the model's ability to capture differences in the average outcome variable between the two groups represented by the indicator variable, while still accounting for the linear relationship between the continuous predictor and the outcome.\n",
    "\n",
    "Comparing the linear forms:\n",
    "\n",
    "Simple Linear Regression (with a continuous predictor):\n",
    "$Y_i$ = $\\beta_0$ + $\\beta_1$ $x_i$ + $\\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}$(0, $\\sigma^2$)\n",
    "\n",
    "Multiple Linear Regression (with a continuous and an indicator predictor):\n",
    "$Y_i$ = $\\beta_A$ + 1_{[x_i=\"B\"]}(x_i)$\\beta_\\textrm{B-offset}$ + $\\beta_z$ $z_i$ + $\\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}$(0, $\\sigma^2$)\n",
    "\n",
    "In the multiple linear regression model:\n",
    "- The indicator variable, denoted as 1_{[x_i=\"B\"]}(x_i), takes the value 1 when the observation belongs to group \"B\" and 0 otherwise.\n",
    "- $\\beta_A$ represents the intercept for the \"baseline\" group (group \"A\" in this case).\n",
    "- $\\beta_\\textrm{B-offset}$ is the contrast coefficient, capturing the difference in the average outcome variable between group \"B\" and group \"A.\"\n",
    "- $\\beta_z$ is the slope coefficient for the continuous predictor variable ($z_i$), which is assumed to be the same for both groups.\n",
    "\n",
    "The multiple linear regression model with an indicator variable essentially fits two parallel lines, one for each group represented by the indicator variable. The lines have the same slope ($\\beta_z$), indicating that the relationship between the continuous predictor and the outcome is consistent across groups. However, the lines have different intercepts, with the difference between the intercepts being equal to the contrast coefficient ($\\beta_\\textrm{B-offset}$). This difference reflects the average difference in the outcome variable between the two groups, after accounting for the effect of the continuous predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c536d4c",
   "metadata": {},
   "source": [
    "### 1.4 The effect of adding an interaction between a continuous and an indicator variable in Multiple Linear Regression models; and this linear form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a3b42b",
   "metadata": {},
   "source": [
    "Adding an interaction between a continuous and an indicator variable in a Multiple Linear Regression model allows the slope of the relationship between the continuous predictor and the outcome to vary depending on the group membership defined by the indicator variable. This results in non-parallel lines, as opposed to the parallel lines observed when only an indicator variable is added alongside a continuous predictor.\n",
    "\n",
    "Multiple Linear Regression (with interaction):\n",
    "\n",
    "$Y_i = \\beta_0 + \\beta_z z_i + \\beta_\\textrm{B-offset} 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\beta_\\textrm{z-change-in-B} z_i \\times 1_{[x_i=\\textrm{\"B\"}]}(x_i) + \\epsilon_i \\quad \\text{where} \\quad \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "- $Y_i$ represents the outcome variable.\n",
    "- $z_i$ represents the continuous predictor.\n",
    "- $x_i$ represents the group membership.\n",
    "- $1_{[x_i=\\textrm{\"B\"}]}(x_i)$ is the indicator variable that takes the value 1 when $x_i$ belongs to group \"B\" and 0 otherwise.\n",
    "- $\\beta_0$ is the intercept for the baseline group (group \"A\").\n",
    "- $\\beta_z$ is the slope of the continuous predictor for the baseline group.\n",
    "- $\\beta_\\textrm{B-offset}$ is the difference in the intercept between group \"B\" and group \"A.\"\n",
    "- $\\beta_\\textrm{z-change-in-B}$ is the difference in the slope of the continuous predictor between group \"B\" and group \"A.\"\n",
    "- $\\epsilon_i$ is the error term, assumed to be normally distributed with a mean of 0 and variance $\\sigma^2$.\n",
    "\n",
    "The effect of the interaction term is captured by $\\beta_\\textrm{z-change-in-B}$.\n",
    "- If $\\beta_\\textrm{z-change-in-B}$ is statistically significant, it indicates that the slope of the relationship between $z_i$ and $Y_i$ is different for group \"B\" compared to group \"A\".\n",
    "- The magnitude of $\\beta_\\textrm{z-change-in-B}$ indicates the extent to which the slope differs between the two groups.\n",
    "\n",
    "This interaction effect can be visualized as two lines with different slopes in a scatter plot of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e88ac0",
   "metadata": {},
   "source": [
    "### 1.5 The behavior of a Multiple Linear Regression model (i.e., the expected nature of the data it models) based only on indicator variables derived from a non-binary categorical variable; this linear form; and the necessarily resulting binary variable encodings it utilizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054098c5",
   "metadata": {},
   "source": [
    "When a multiple linear regression model is based solely on indicator variables derived from a non-binary categorical variable, it essentially performs an analysis of variance (ANOVA). This allows for comparing the average outcome variable across multiple groups defined by the categorical variable.\n",
    "\n",
    "The linear form for such a model can be represented as:\n",
    "\n",
    "$Y_i = \\beta_0 + \\beta_1 1_{[x_i = \"B\"]}(x_i) + \\beta_2 1_{[x_i = \"C\"]}(x_i) + ... + \\beta_{K-1} 1_{[x_i = \"K\"]}(x_i) + \\epsilon_i$ where $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "where:\n",
    "\n",
    "- $Y_i$ represents the outcome variable.\n",
    "- $x_i$ represents the categorical variable with K levels (e.g., \"A\", \"B\", \"C\", ..., \"K\").\n",
    "- $1_{[x_i = \"Level\"]}(x_i)$ represents an indicator variable that takes the value 1 if $x_i$ belongs to the specified \"Level\" and 0 otherwise.\n",
    "- $\\beta_0$ represents the intercept, which is the average outcome for the baseline group (usually the group omitted from the indicator variables).\n",
    "- $\\beta_1$, $\\beta_2$, ..., $\\beta_{K-1}$ represent the contrast coefficients, capturing the average differences in the outcome variable between each group and the baseline group.\n",
    "- $\\epsilon_i$ is the error term.\n",
    "\n",
    "To use a non-binary categorical variable in a multiple linear regression, it needs to be encoded into K-1 binary indicator variables. This encoding scheme creates a set of dummy variables where each represents one level of the categorical variable, excluding the baseline level.\n",
    "\n",
    "For instance, if the categorical variable $x_i$ has three levels (\"A\", \"B\", and \"C\"), you would create two binary indicator variables:\n",
    "- $1_{[x_i = \"B\"]}(x_i)$: Takes the value 1 if $x_i$ is \"B\" and 0 otherwise.\n",
    "- $1_{[x_i = \"C\"]}(x_i)$: Takes the value 1 if $x_i$ is \"C\" and 0 otherwise.\n",
    "\n",
    "Notice that there isn't an indicator variable for level \"A\" because it serves as the baseline group. When both indicator variables are 0, it implicitly implies that the observation belongs to group \"A.\"\n",
    "\n",
    "The model assumes that the outcome variable is a continuous numeric variable and is linearly related to the categorical variable. This means that the model estimates a different average outcome value for each group represented by the indicator variables.\n",
    "\n",
    "Visually, this model can be depicted as a set of horizontal lines, one for each group, where the height of each line represents the average outcome value for that group. The differences in heights between the lines represent the estimated effects of the categorical variable on the outcome. The data points are scattered around these horizontal lines, reflecting the variability within each group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acb451",
   "metadata": {},
   "source": [
    "NBLM Summary:\n",
    "\n",
    "This conversation explored the impact of incorporating indicator variables and interactions into Multiple Linear Regression models.\n",
    "\n",
    "Initially, you asked about the change in model behavior when a single indicator variable is introduced alongside a continuous variable. The response highlighted the shift from a single straight line to two parallel lines, reflecting the model's ability to capture differences in the average outcome variable between the two groups represented by the indicator variable while accounting for the linear relationship with the continuous predictor.\n",
    "\n",
    "Next, you inquired about the effect of adding an interaction between a continuous and indicator variable. The explanation emphasized that the interaction allows the slope of the relationship between the continuous predictor and the outcome to vary depending on the group defined by the indicator variable, resulting in non-parallel lines.\n",
    "\n",
    "Finally, you asked about the behavior of a multiple linear regression model based only on indicator variables derived from a non-binary categorical variable. The response clarified that such a model essentially performs an ANOVA, comparing the average outcome across multiple groups defined by the categorical variable. This involves encoding the categorical variable into K-1 binary indicator variables, where each represents one level excluding the baseline. The resulting model estimates a different average outcome value for each group, visually depicted as a set of horizontal lines.\n",
    "\n",
    "The sources provided context for these concepts within the framework of a data science course at the University of Toronto. The course emphasizes using ChatBots as interactive learning tools alongside traditional resources like Jupyter notebooks and GitHub. The homework assignments encourage students to explore various statistical techniques, including data summarization, visualization, bootstrapping, hypothesis testing, and linear regression.\n",
    "\n",
    "While the sources don't directly address the specific questions about indicator variables and interactions in multiple linear regression, they do highlight the importance of understanding these concepts for data analysis. For example, the homework assignments require students to work with categorical variables and create visualizations to explore potential associations. These tasks necessitate an understanding of how to represent categorical variables in regression models and interpret the resulting output. \n",
    "\n",
    "The sources also stress the value of using ChatBots to clarify concepts and gain a deeper understanding of the material. This reinforces the approach taken in this conversation, where explanations and examples were provided to enhance your understanding of indicator variables and interactions in multiple linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bdd11e",
   "metadata": {},
   "source": [
    "## 2. The specific (outcome and predictor) variables for the scenario below; whether or not any meaningful interactions might need to be taken into account when predicting the outcome; and the linear forms with and without the potential interactions that might need to be considered\n",
    "\n",
    "The outcome variable is the effectiveness of the advertising campaigns, which could be measured in terms of sales revenue, website traffic, or brand awareness.\n",
    "\n",
    "The predictor variables are the amount spent on TV advertising and the amount spent on online advertising.\n",
    "\n",
    "It might be important to take into account the interaction between the two advertising mediums when predicting the outcome. This is because the effectiveness of one medium might depend on the amount spent on the other. For example, a TV ad might be more effective if it is accompanied by a strong online campaign.\n",
    "\n",
    "Linear forms for the model with and without the interaction:\n",
    "\n",
    "Without interaction:\n",
    "\n",
    "Outcome = β<sub>0</sub> + β<sub>TV</sub>(TV advertising) + β<sub>online</sub>(online advertising) \n",
    "\n",
    "With interaction:\n",
    "\n",
    "Outcome = β<sub>0</sub> + β<sub>TV</sub>(TV advertising) + β<sub>online</sub>(online advertising) + β<sub>interaction</sub>(TV advertising x online advertising)\n",
    "\n",
    "Explanation of Linear Forms:\n",
    "- The β0 is the intercept and represents the expected outcome when both TV and online advertising spending are zero.\n",
    "- The βTV and βonline are the coefficients for the TV advertising and online advertising variables, respectively, and represent the expected change in the outcome for a one-unit increase in the corresponding predictor variable, holding the other predictor variable constant.\n",
    "- The βinteraction is the coefficient for the interaction term and represents the additional change in the outcome when both TV advertising and online advertising are increased by one unit.\n",
    "\n",
    "Predictions:\n",
    "- To make predictions using these formulas, simply plug in the values for the predictor variables and calculate the outcome.\n",
    "- For the model without the interaction, the effect of each advertising medium is assumed to be independent of the other.\n",
    "- For the model with the interaction, the effect of each advertising medium is allowed to depend on the level of the other.\n",
    "- The interaction term captures the synergistic effect of the two advertising mediums."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf61320b",
   "metadata": {},
   "source": [
    "### 2.1 Explanation of how to use these two formulas to make predictions of the outcome, and give a high level explaination in general terms of the difference between predictions from the models with and without the interaction\n",
    "\n",
    "To make predictions using the two formulas, values for the predictor variables (TV advertising spending and online advertising spending) would be plugged in. The outcome variable would be the effectiveness of the advertising campaign.\n",
    "\n",
    "Without the interaction: The effect of each advertising medium on the outcome is assumed to be independent of the other medium. In other words, spending more on TV advertising would always have the same effect on the outcome, regardless of how much is spent on online advertising.\n",
    "\n",
    "With the interaction: The effect of each advertising medium is allowed to depend on the level of the other medium. This means that, for instance, the effect of TV advertising spending on the outcome could change based on online advertising spending. The interaction term captures the synergistic effect of the two mediums.\n",
    "\n",
    "In general terms, the model with the interaction is more flexible than the model without the interaction. This is because it can capture the possibility that the effect of one predictor variable on the outcome depends on the value of another predictor variable. However, the model with the interaction is also more complex and can be more difficult to interpret."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f63ef2a",
   "metadata": {},
   "source": [
    "### 2.2 Explanationof how to update and use the implied two formulas to make predictions of the outcome if, rather than considering two continuous predictor variables, we instead suppose the advertisement budgets are simply categorized as either \"high\" or \"low\" (binary variables)\n",
    "\n",
    "When advertising budgets are categorized as either \"high\" or \"low,\" the predictor variables become binary. To represent these binary variables in the formulas, indicator variables can be used.\n",
    "\n",
    "For example, let:\n",
    "- TV_high = 1 if TV advertising budget is high, 0 otherwise\n",
    "- online_high = 1 if online advertising budget is high, 0 otherwise\n",
    "\n",
    "Then, the linear forms of the model become:\n",
    "\n",
    "Without interaction:\n",
    "\n",
    "Outcome = β<sub>0</sub> + β<sub>TV</sub>(TV_high) + β<sub>online</sub>(online_high)\n",
    "\n",
    "With interaction:\n",
    "\n",
    "Outcome = β<sub>0</sub> + β<sub>TV</sub>(TV_high) + β<sub>online</sub>(online_high) + β<sub>interaction</sub>(TV_high x online_high)\n",
    "\n",
    "Predictions with Binary Variables:\n",
    "\n",
    "To make predictions using these updated formulas, we would follow these steps:\n",
    "\n",
    "1. Determine the values of the indicator variables: For a given observation, determine whether the TV and online advertising budgets are high or low. Assign the corresponding values (1 or 0) to the indicator variables TV_high and online_high.\n",
    "2. Plug in the values and calculate the outcome: Substitute the values of the indicator variables and the estimated coefficients (obtained from fitting the model to the data) into the appropriate formula. Calculate the outcome, which represents the predicted effectiveness of the advertising campaign.\n",
    "\n",
    "Understanding the Interaction:\n",
    "\n",
    "Consider the four possible combinations of the binary variables:\n",
    "- TV_high = 0, online_high = 0 (both budgets low)\n",
    "- TV_high = 1, online_high = 0 (TV budget high, online budget low)\n",
    "- TV_high = 0, online_high = 1 (TV budget low, online budget high)\n",
    "- TV_high = 1, online_high = 1 (both budgets high)\n",
    "\n",
    "The interaction term only comes into play when both TV_high and online_high are 1 (both budgets high). In this case, the interaction term contributes an additional effect to the predicted outcome, capturing the synergistic effect of having both budgets high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66db6b8",
   "metadata": {},
   "source": [
    "NBLM Summary:\n",
    "\n",
    "### Summary of Advertising Campaign Effectiveness\n",
    "\n",
    "The conversation focused on understanding the outcome and predictor variables in an advertising campaign scenario where a company sells sports equipment and runs campaigns on TV and online platforms. The conversation explored how to represent these variables in linear models and how to make predictions using those models.\n",
    "\n",
    "Initially, the conversation focused on the scenario where the amount spent on TV and online advertising were treated as continuous predictor variables. The outcome variable was identified as the effectiveness of the advertising campaign, which could be measured in terms of sales, website traffic, or brand awareness. Two linear forms of the model were presented: one without interaction between the two advertising mediums and one with interaction.\n",
    "\n",
    "*   The model **without interaction** assumes that the effect of each advertising medium on the outcome is independent of the other.\n",
    "*   The model **with interaction** allows the effect of one advertising medium to depend on the level of the other.\n",
    "\n",
    "The conversation then shifted to explore how to update and use the formulas to make predictions if the advertising budgets were categorized as \"high\" or \"low\" (binary variables). Indicator variables were introduced to represent these binary variables in the models. The updated linear forms of the models were presented, and the steps for making predictions using these updated formulas were outlined. The conversation highlighted the difference in predictions between models with and without interaction in this scenario.\n",
    "\n",
    "The sources provide examples of using Python to create simulated datasets and visualize linear models, which can help in understanding how the interaction term affects the relationship between predictor and outcome variables. However, the sources do not provide specific details on updating the formulas with binary variables. The explanation provided in the conversation for this scenario is based on a general understanding of indicator variables and interactions in linear models. You may want to independently verify this information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc7d797",
   "metadata": {},
   "source": [
    "## 3. Using smf to fit multiple linear regression models to the course project dataset from the canadian social connection survey\n",
    "\n",
    "Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eebc83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.683805\n",
      "         Iterations 4\n",
      "                            Logit Regression Results                            \n",
      "================================================================================\n",
      "Dep. Variable:     GEO_residence_canada   No. Observations:                 1000\n",
      "Model:                            Logit   Df Residuals:                      997\n",
      "Method:                             MLE   Df Model:                            2\n",
      "Date:                  Thu, 14 Nov 2024   Pseudo R-squ.:               0.0004927\n",
      "Time:                          03:23:21   Log-Likelihood:                -683.81\n",
      "converged:                         True   LL-Null:                       -684.14\n",
      "Covariance Type:              nonrobust   LLR p-value:                    0.7138\n",
      "===============================================================================================================\n",
      "                                                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "---------------------------------------------------------------------------------------------------------------\n",
      "Intercept                                      -0.3196      0.220     -1.450      0.147      -0.752       0.112\n",
      "DEMO_age                                        0.0023      0.004      0.639      0.523      -0.005       0.009\n",
      "LONELY_existential_loneliness_scale_outlook    -0.0115      0.022     -0.526      0.599      -0.055       0.031\n",
      "===============================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Simulating a dataset for logistic regression\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a simulated dataset\n",
    "n_samples = 1000\n",
    "\n",
    "# Simulated continuous predictor: DEMO_age (ages between 18 and 80)\n",
    "DEMO_age = np.random.randint(18, 80, size=n_samples)\n",
    "\n",
    "# Simulated binary predictor: LONELY_existential_loneliness_scale_outlook (1 to 10 scale)\n",
    "LONELY_existential_loneliness_scale_outlook = np.random.randint(1, 11, size=n_samples)\n",
    "\n",
    "# Simulated target variable: GEO_residence_canada (Yes/No) with some NA values\n",
    "GEO_residence_canada = np.random.choice([1, 0, np.nan], size=n_samples, p=[0.45, 0.45, 0.1])\n",
    "\n",
    "# Create a dataframe\n",
    "data_simulated = pd.DataFrame({\n",
    "    'DEMO_age': DEMO_age,\n",
    "    'LONELY_existential_loneliness_scale_outlook': LONELY_existential_loneliness_scale_outlook,\n",
    "    'GEO_residence_canada': GEO_residence_canada\n",
    "})\n",
    "\n",
    "# Fill missing values in 'GEO_residence_canada' with mode (1 = Yes, 0 = No)\n",
    "data_simulated['GEO_residence_canada'] = data_simulated['GEO_residence_canada'].fillna(data_simulated['GEO_residence_canada'].mode()[0])\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logit_formula = 'GEO_residence_canada ~ DEMO_age + LONELY_existential_loneliness_scale_outlook'\n",
    "logit_model = smf.logit(logit_formula, data=data_simulated).fit()\n",
    "\n",
    "# Output the results\n",
    "print(logit_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d33dba9",
   "metadata": {},
   "source": [
    "Explanation, provided by ChatGPT:\n",
    "\n",
    "The process of conducting logistic regression with this simulated dataset involves several steps. Let me break it down for you:\n",
    "\n",
    "### 1. **Simulating the Dataset**\n",
    "   - **Predictor Variables**: These are the independent variables (features) you use to predict the target variable.\n",
    "     - **DEMO_age**: A continuous predictor representing the age of participants. It's simulated as a random integer between 18 and 80 using `np.random.randint()`.\n",
    "     - **LONELY_existential_loneliness_scale_outlook**: A categorical predictor (from a scale of 1 to 10) representing how lonely a person feels existentially. Again, it's simulated as a random integer between 1 and 10.\n",
    "\n",
    "   - **Target Variable**: This is the dependent variable that you are trying to predict.\n",
    "     - **GEO_residence_canada**: A binary target variable (1 for \"Yes\", 0 for \"No\"). This is simulated as a random choice between 1 (Yes), 0 (No), and `np.nan` (missing values), with probabilities of 45% for \"Yes\", 45% for \"No\", and 10% for missing (`np.nan`).\n",
    "\n",
    "### 2. **Data Preparation**\n",
    "   - **Handling Missing Data**: The `GEO_residence_canada` variable has missing values (`np.nan`). To make sure the logistic regression can be applied (since it cannot handle missing values directly), we fill these missing values with the **mode** of the `GEO_residence_canada` column. The mode is the most common value (in this case, either 1 or 0).\n",
    "   \n",
    "   - **Final Dataset**: After filling missing values, we have a complete dataset where:\n",
    "     - `DEMO_age` is continuous.\n",
    "     - `LONELY_existential_loneliness_scale_outlook` is categorical (scale from 1 to 10).\n",
    "     - `GEO_residence_canada` is binary, with no missing values.\n",
    "\n",
    "### 3. **Fitting the Logistic Regression Model**\n",
    "   - **Formula**: The formula for logistic regression is a mathematical expression that describes how the target variable is predicted from the predictors. The formula is written as:\n",
    "     ```\n",
    "     GEO_residence_canada ~ DEMO_age + LONELY_existential_loneliness_scale_outlook\n",
    "     ```\n",
    "     This means that `GEO_residence_canada` (the target variable) is predicted based on `DEMO_age` and `LONELY_existential_loneliness_scale_outlook`.\n",
    "   \n",
    "   - **Logistic Regression**: The logistic regression model is fit using the `smf.logit()` function from the `statsmodels` library. The function fits a model to estimate the relationship between the target and predictor variables using the logistic function (sigmoid curve).\n",
    "   \n",
    "   - **Fitting Process**: The `fit()` method estimates the model parameters (coefficients) by maximizing the likelihood function. The model uses an iterative process (usually Newton-Raphson or similar optimization algorithms) to adjust the coefficients of the predictors to make the predictions most consistent with the observed data.\n",
    "\n",
    "### 4. **Output - Model Summary**\n",
    "   - After the model is fitted, we call `logit_model.summary()` to print out the results of the regression analysis. This summary includes important details:\n",
    "     - **Coefficients**: These represent the effect of each predictor on the log-odds of the target variable. For example, a positive coefficient means that as the predictor increases, the likelihood of the target being \"Yes\" increases, and vice versa.\n",
    "     - **p-values**: These help you assess whether the coefficients are statistically significant (usually, a p-value < 0.05 indicates statistical significance).\n",
    "     - **Confidence Intervals**: These give you a range of values that the true coefficient might fall within, with a certain level of confidence (usually 95%).\n",
    "     - **Goodness of Fit**: Measures like **AIC** (Akaike Information Criterion) and **Pseudo R-squared** help evaluate how well the model fits the data.\n",
    "\n",
    "### 5. **Interpreting Results**\n",
    "   - **Coefficients**: Each predictor has an associated coefficient. The sign of the coefficient tells you the direction of the relationship (positive or negative). The magnitude tells you how strong that relationship is.\n",
    "   - **Odds Ratio**: The coefficients can be exponentiated to calculate the odds ratio, which gives a more intuitive interpretation of the relationship. For example, if the coefficient for `DEMO_age` is 0.05, the odds of being a \"Yes\" for each year of age would increase by a factor of `exp(0.05) ≈ 1.051`, meaning for each year of age, the odds of living in Canada would increase by 5.1%.\n",
    "   \n",
    "### 6. **Final Model**\n",
    "   - Once you have the coefficients and other statistical measures, you can use the fitted model to predict the target variable (`GEO_residence_canada`) for new data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31005a55",
   "metadata": {},
   "source": [
    "ChatGPT Summary:\n",
    "\n",
    "Sure! Here's a summary of our conversation:\n",
    "\n",
    "1. **Logistic Regression Setup**:\n",
    "   - We discussed your assignment involving logistic regression, specifically predicting whether a person lives in Canada (`GEO_residence_canada`) based on their age (`DEMO_age`) and existential loneliness score (`LONELY_existential_loneliness_scale_outlook`).\n",
    "   - Initially, the dataset had some missing values in `GEO_residence_canada`, which were handled by filling those with the mode of the column (the most frequent value, either 1 or 0).\n",
    "   - I provided you with simulated data, which included continuous and categorical variables for logistic regression.\n",
    "\n",
    "2. **Data Preparation**:\n",
    "   - We prepared the dataset by filling in missing values in the target variable (`GEO_residence_canada`) and ensured that the predictors (`DEMO_age`, `LONELY_existential_loneliness_scale_outlook`) were properly formatted for logistic regression.\n",
    "   - After cleaning the data, the logistic regression model was fit using the formula `GEO_residence_canada ~ DEMO_age + LONELY_existential_loneliness_scale_outlook`.\n",
    "\n",
    "3. **Fitting the Model**:\n",
    "   - I guided you through the process of using the `statsmodels` library's `logit()` function to fit the logistic regression model, explaining that the model calculates the relationship between the target and predictor variables by maximizing the likelihood function.\n",
    "   - The logistic regression model uses the logistic function (sigmoid) to predict the probability of the target variable (`GEO_residence_canada`).\n",
    "\n",
    "4. **Model Interpretation**:\n",
    "   - We discussed interpreting the logistic regression results, including the model summary with coefficients, p-values, and confidence intervals. The coefficients indicate the direction and strength of the relationship between the predictors and the target.\n",
    "   - The odds ratio (obtained by exponentiating the coefficients) helps in interpreting the effect of each predictor in terms of the odds of the target variable.\n",
    "\n",
    "5. **Code Execution**:\n",
    "   - I provided a code snippet to simulate data, handle missing values, and run the logistic regression model. You can apply this code to your real dataset once it's cleaned and structured similarly.\n",
    "\n",
    "6. **Final Notes**:\n",
    "   - I offered guidance on using the logistic regression model to predict the target variable for new observations.\n",
    "   - You were provided with an overview of the steps and the logistic regression process, including how to interpret the model's output.\n",
    "\n",
    "Let me know if you'd like any additional details or further clarification!\n",
    "\n",
    "ChatGPT Chatlog link: https://chatgpt.com/share/67356dc1-8420-8004-ab9d-f09c799d3d5f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f156e0a",
   "metadata": {},
   "source": [
    "## 4. Explanation of the apparent contradiction between the factual statements regarding the fit below that \"the model only explains 17.6% of the variability in the data\" while at the same time \"many of the coefficients are larger than 10 while having strong or very strong evidence against the null hypothesis of 'no effect'\"\n",
    "\n",
    "The fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4e893c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Bulbasaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>45</td>\n",
       "      <td>49</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ivysaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>83</td>\n",
       "      <td>100</td>\n",
       "      <td>100</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VenusaurMega Venusaur</td>\n",
       "      <td>Grass</td>\n",
       "      <td>Poison</td>\n",
       "      <td>80</td>\n",
       "      <td>100</td>\n",
       "      <td>123</td>\n",
       "      <td>122</td>\n",
       "      <td>120</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Charmander</td>\n",
       "      <td>Fire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>719</td>\n",
       "      <td>Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>150</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>719</td>\n",
       "      <td>DiancieMega Diancie</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Fairy</td>\n",
       "      <td>50</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>160</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Unbound</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Dark</td>\n",
       "      <td>80</td>\n",
       "      <td>160</td>\n",
       "      <td>60</td>\n",
       "      <td>170</td>\n",
       "      <td>130</td>\n",
       "      <td>80</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>721</td>\n",
       "      <td>Volcanion</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Water</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>120</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                   Name   Type 1  Type 2  HP  Attack  Defense  \\\n",
       "0      1              Bulbasaur    Grass  Poison  45      49       49   \n",
       "1      2                Ivysaur    Grass  Poison  60      62       63   \n",
       "2      3               Venusaur    Grass  Poison  80      82       83   \n",
       "3      3  VenusaurMega Venusaur    Grass  Poison  80     100      123   \n",
       "4      4             Charmander     Fire     NaN  39      52       43   \n",
       "..   ...                    ...      ...     ...  ..     ...      ...   \n",
       "795  719                Diancie     Rock   Fairy  50     100      150   \n",
       "796  719    DiancieMega Diancie     Rock   Fairy  50     160      110   \n",
       "797  720    HoopaHoopa Confined  Psychic   Ghost  80     110       60   \n",
       "798  720     HoopaHoopa Unbound  Psychic    Dark  80     160       60   \n",
       "799  721              Volcanion     Fire   Water  80     110      120   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "0         65       65     45           1      False  \n",
       "1         80       80     60           1      False  \n",
       "2        100      100     80           1      False  \n",
       "3        122      120     80           1      False  \n",
       "4         60       50     65           1      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "795      100      150     50           6       True  \n",
       "796      160      110    110           6       True  \n",
       "797      150      130     70           6       True  \n",
       "798      170      130     80           6       True  \n",
       "799      130       90     70           6       True  \n",
       "\n",
       "[800 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/KeithGalli/pandas/master/pokemon_data.csv\"\n",
    "# fail https://github.com/KeithGalli/pandas/blob/master/pokemon_data.csv\n",
    "pokeaman = pd.read_csv(url) \n",
    "pokeaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a8f7427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   15.27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>3.50e-27</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:10:40</td>     <th>  Log-Likelihood:    </th> <td> -3649.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>   7323.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   788</td>      <th>  BIC:               </th> <td>   7379.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                       <td>   26.8971</td> <td>    5.246</td> <td>    5.127</td> <td> 0.000</td> <td>   16.599</td> <td>   37.195</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>              <td>   20.0449</td> <td>    7.821</td> <td>    2.563</td> <td> 0.011</td> <td>    4.692</td> <td>   35.398</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>              <td>   21.3662</td> <td>    6.998</td> <td>    3.053</td> <td> 0.002</td> <td>    7.629</td> <td>   35.103</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>              <td>   31.9575</td> <td>    8.235</td> <td>    3.881</td> <td> 0.000</td> <td>   15.793</td> <td>   48.122</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>              <td>    9.4926</td> <td>    7.883</td> <td>    1.204</td> <td> 0.229</td> <td>   -5.982</td> <td>   24.968</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>              <td>   22.2693</td> <td>    8.709</td> <td>    2.557</td> <td> 0.011</td> <td>    5.173</td> <td>   39.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                    <td>    0.5634</td> <td>    0.071</td> <td>    7.906</td> <td> 0.000</td> <td>    0.423</td> <td>    0.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.2]</th> <td>   -0.2350</td> <td>    0.101</td> <td>   -2.316</td> <td> 0.021</td> <td>   -0.434</td> <td>   -0.036</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.3]</th> <td>   -0.3067</td> <td>    0.093</td> <td>   -3.300</td> <td> 0.001</td> <td>   -0.489</td> <td>   -0.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.4]</th> <td>   -0.3790</td> <td>    0.105</td> <td>   -3.600</td> <td> 0.000</td> <td>   -0.586</td> <td>   -0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.5]</th> <td>   -0.0484</td> <td>    0.108</td> <td>   -0.447</td> <td> 0.655</td> <td>   -0.261</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):C(Generation)[T.6]</th> <td>   -0.3083</td> <td>    0.112</td> <td>   -2.756</td> <td> 0.006</td> <td>   -0.528</td> <td>   -0.089</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>337.229</td> <th>  Durbin-Watson:     </th> <td>   1.505</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2871.522</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 1.684</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>11.649</td>  <th>  Cond. No.          </th> <td>1.40e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.4e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                  &        HP        & \\textbf{  R-squared:         } &     0.176   \\\\\n",
       "\\textbf{Model:}                          &       OLS        & \\textbf{  Adj. R-squared:    } &     0.164   \\\\\n",
       "\\textbf{Method:}                         &  Least Squares   & \\textbf{  F-statistic:       } &     15.27   \\\\\n",
       "\\textbf{Date:}                           & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  3.50e-27   \\\\\n",
       "\\textbf{Time:}                           &     21:10:40     & \\textbf{  Log-Likelihood:    } &   -3649.4   \\\\\n",
       "\\textbf{No. Observations:}               &         800      & \\textbf{  AIC:               } &     7323.   \\\\\n",
       "\\textbf{Df Residuals:}                   &         788      & \\textbf{  BIC:               } &     7379.   \\\\\n",
       "\\textbf{Df Model:}                       &          11      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                         & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                       &      26.8971  &        5.246     &     5.127  &         0.000        &       16.599    &       37.195     \\\\\n",
       "\\textbf{C(Generation)[T.2]}              &      20.0449  &        7.821     &     2.563  &         0.011        &        4.692    &       35.398     \\\\\n",
       "\\textbf{C(Generation)[T.3]}              &      21.3662  &        6.998     &     3.053  &         0.002        &        7.629    &       35.103     \\\\\n",
       "\\textbf{C(Generation)[T.4]}              &      31.9575  &        8.235     &     3.881  &         0.000        &       15.793    &       48.122     \\\\\n",
       "\\textbf{C(Generation)[T.5]}              &       9.4926  &        7.883     &     1.204  &         0.229        &       -5.982    &       24.968     \\\\\n",
       "\\textbf{C(Generation)[T.6]}              &      22.2693  &        8.709     &     2.557  &         0.011        &        5.173    &       39.366     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                    &       0.5634  &        0.071     &     7.906  &         0.000        &        0.423    &        0.703     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.2]} &      -0.2350  &        0.101     &    -2.316  &         0.021        &       -0.434    &       -0.036     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.3]} &      -0.3067  &        0.093     &    -3.300  &         0.001        &       -0.489    &       -0.124     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.4]} &      -0.3790  &        0.105     &    -3.600  &         0.000        &       -0.586    &       -0.172     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.5]} &      -0.0484  &        0.108     &    -0.447  &         0.655        &       -0.261    &        0.164     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):C(Generation)[T.6]} &      -0.3083  &        0.112     &    -2.756  &         0.006        &       -0.528    &       -0.089     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 337.229 & \\textbf{  Durbin-Watson:     } &    1.505  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2871.522  \\\\\n",
       "\\textbf{Skew:}          &   1.684 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  11.649 & \\textbf{  Cond. No.          } & 1.40e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.4e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.176\n",
       "Model:                            OLS   Adj. R-squared:                  0.164\n",
       "Method:                 Least Squares   F-statistic:                     15.27\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           3.50e-27\n",
       "Time:                        21:10:40   Log-Likelihood:                -3649.4\n",
       "No. Observations:                 800   AIC:                             7323.\n",
       "Df Residuals:                     788   BIC:                             7379.\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===================================================================================================\n",
       "                                      coef    std err          t      P>|t|      [0.025      0.975]\n",
       "---------------------------------------------------------------------------------------------------\n",
       "Intercept                          26.8971      5.246      5.127      0.000      16.599      37.195\n",
       "C(Generation)[T.2]                 20.0449      7.821      2.563      0.011       4.692      35.398\n",
       "C(Generation)[T.3]                 21.3662      6.998      3.053      0.002       7.629      35.103\n",
       "C(Generation)[T.4]                 31.9575      8.235      3.881      0.000      15.793      48.122\n",
       "C(Generation)[T.5]                  9.4926      7.883      1.204      0.229      -5.982      24.968\n",
       "C(Generation)[T.6]                 22.2693      8.709      2.557      0.011       5.173      39.366\n",
       "Q(\"Sp. Def\")                        0.5634      0.071      7.906      0.000       0.423       0.703\n",
       "Q(\"Sp. Def\"):C(Generation)[T.2]    -0.2350      0.101     -2.316      0.021      -0.434      -0.036\n",
       "Q(\"Sp. Def\"):C(Generation)[T.3]    -0.3067      0.093     -3.300      0.001      -0.489      -0.124\n",
       "Q(\"Sp. Def\"):C(Generation)[T.4]    -0.3790      0.105     -3.600      0.000      -0.586      -0.172\n",
       "Q(\"Sp. Def\"):C(Generation)[T.5]    -0.0484      0.108     -0.447      0.655      -0.261       0.164\n",
       "Q(\"Sp. Def\"):C(Generation)[T.6]    -0.3083      0.112     -2.756      0.006      -0.528      -0.089\n",
       "==============================================================================\n",
       "Omnibus:                      337.229   Durbin-Watson:                   1.505\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2871.522\n",
       "Skew:                           1.684   Prob(JB):                         0.00\n",
       "Kurtosis:                      11.649   Cond. No.                     1.40e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.4e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "model1_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") + C(Generation) + Q(\"Sp. Def\"):C(Generation)', data=pokeaman)\n",
    "model2_spec = smf.ols(formula='HP ~ Q(\"Sp. Def\") * C(Generation)', data=pokeaman)\n",
    "\n",
    "model2_fit = model2_spec.fit()\n",
    "model2_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739d180",
   "metadata": {},
   "source": [
    "R-squared (the proportion of variation in the outcome explained by the model) is a measure of a model's overall explanatory power. It represents how well the model, as a whole, fits the data. A low R-squared value, such as 17.6%, means that the model does not explain a large portion of the variability in the outcome variable.\n",
    "\n",
    "On the other hand, p-values and the magnitude of coefficients assess the individual effects of predictor variables within the model. A low p-value and a large coefficient for a specific predictor variable indicate that the variable has a statistically significant and substantial effect on the outcome variable, holding all other predictor variables constant.\n",
    "\n",
    "There may be reasons why the contradiction may happen, as described below:\n",
    "\n",
    "- Unaccounted Variance: A low R-squared indicates a significant portion of the variance in the outcome variable (HP in this case) remains unexplained by the model. This unaccounted variance could stem from:\n",
    "    - Omitted Variable Bias: The model only considers \"Sp. Def\" and \"Generation\". There could be other relevant factors influencing HP that are not included in the model. These omitted variables could directly affect HP or be correlated with the existing predictors, leading to biased estimates.\n",
    "    - Non-linearity: The model assumes a linear relationship between predictors and HP. If the true relationship is non-linear (e.g., quadratic or exponential), the model will not capture it fully, resulting in lower explanatory power.\n",
    "    - Interactions Beyond the Model: The model includes an interaction term between \"Sp. Def\" and \"Generation.\" However, more complex interactions, like three-way interactions or interactions with omitted variables, might exist, contributing to the unexplained variance.\n",
    "- Nature of Statistical Significance: It's crucial to recognize that statistical significance (indicated by low p-values) doesn't necessarily imply practical significance.\n",
    "    - Large Sample Sizes: With large sample sizes, even small effects can become statistically significant. The Pokemon dataset likely has a substantial number of observations, potentially making even relatively weak effects detectable.\n",
    "    - Magnitude Over Significance: While large coefficients might seem impactful, their practical relevance depends on the scale of the variables. It's important to consider the context and the magnitude of the effect in relation to the variable's units.\n",
    "- Model Complexity and Overfitting: Including many predictors or interaction terms increases model complexity. While this can improve R-squared on the training data, it can lead to overfitting, where the model captures noise rather than true patterns. This overfitting might result in a model that performs well on the training data but poorly on new data.\n",
    "\n",
    "\n",
    "In essence, the observed situation emphasizes that a statistically significant model doesn't guarantee a high R-squared. The significant predictors highlight specific, potentially meaningful relationships, while the low R-squared indicates that the model, in its current form, does not fully encapsulate all the factors driving HP variation. Addressing omitted variables, exploring non-linear relationships, and cautiously managing model complexity are crucial steps for improving explanatory power and building more robust models.\n",
    "\n",
    "In practical terms, a low R-squared suggests caution in using the model for predictive purposes but doesn’t undermine the importance or reliability of the identified relationships between predictors and the outcome. A low R-squared Shows that the model doesn't capture all the influences on HP. It’s simply a measure of overall fit, and even with a low overall R-squared, individual predictors can still have a strong and reliable impact on the dependent variable. This indicates that the relationship between these predictors and HP is robust, even if other factors are at play."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e16531",
   "metadata": {},
   "source": [
    "NBLM Summary:\n",
    "\n",
    "### Summary of Conversation: Exploring the Relationship Between R-squared and Statistical Significance\n",
    "\n",
    "This conversation focused on understanding the apparent contradiction between a multiple linear regression model having a low R-squared value (17.6%) while simultaneously possessing statistically significant predictor variables with large coefficients.\n",
    "\n",
    "*   **Initial Observation:** You presented a multiple linear regression model (`model2_fit`) using data from the 'pokemon' dataset, predicting HP based on \"Sp. Def\" and \"Generation\", including their interaction. The model exhibited a low R-squared, indicating it only explained a small portion of the variability in HP. However, the model's summary showed several statistically significant predictors with coefficients larger than 10.\n",
    "\n",
    "*   **Explanation:** You clarified that R-squared, as a measure of overall model fit, reflects how well the model as a whole explains the variance in the outcome variable. It's distinct from p-values and coefficient magnitudes, which assess the individual effects of predictors within the model. A model can have statistically significant predictors while still having a low R-squared due to factors such as omitted variables, non-linear relationships, random noise, or overfitting.\n",
    "\n",
    "*   **Technical Nuances:**  You requested further elaboration on the technical aspects behind this observation. While acknowledging the limitations of the provided sources in addressing these technical details directly, a response was generated based on the conversation history and general statistical knowledge, highlighting several potential contributing factors:\n",
    "    *   **Unexplained Variance:** The low R-squared pointed to unaccounted variance in HP. Potential sources include omitted variables, non-linear relationships not captured by the linear model, or more complex interactions beyond those included in the model.\n",
    "    *   **Statistical vs. Practical Significance:** The distinction between statistical and practical significance was emphasized. The large sample size of the Pokemon dataset could make statistically significant results arise even from relatively weak effects. Furthermore, the magnitude of coefficients needs to be interpreted within the context of the variables' scales to determine practical relevance.\n",
    "    *   **Model Complexity and Overfitting:**  The potential for overfitting when including many predictors or interaction terms was also brought up. Overfitting can lead to inflated R-squared on training data but poor generalization to new data.\n",
    "\n",
    "*   **Key Takeaway:** The conversation highlighted that a statistically significant model does not inherently guarantee a high R-squared value. The presence of statistically significant predictors with large coefficients signifies potentially meaningful relationships. However, the low R-squared suggests the need to explore other potential factors influencing HP and to cautiously manage model complexity to ensure robust and generalizable findings.\n",
    "\n",
    "**It's essential to remember that this explanation draws upon general statistical knowledge and builds upon the information exchanged during the conversation. The sources provided do not contain specific information about the technical aspects discussed.** Therefore, it is recommended to verify these interpretations with additional resources or through further exploration with a chatbot of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a56edd",
   "metadata": {},
   "source": [
    "## 5. Discussion of the following (five cells of) code and results with a ChatBot and explanation of what the following (five cells of) are illustrating based on the understanding I arrive at in this conversation\n",
    "\n",
    "Code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9d59eeb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#</th>\n",
       "      <th>Name</th>\n",
       "      <th>Type 1</th>\n",
       "      <th>Type 2</th>\n",
       "      <th>HP</th>\n",
       "      <th>Attack</th>\n",
       "      <th>Defense</th>\n",
       "      <th>Sp. Atk</th>\n",
       "      <th>Sp. Def</th>\n",
       "      <th>Speed</th>\n",
       "      <th>Generation</th>\n",
       "      <th>Legendary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>338</td>\n",
       "      <td>Solrock</td>\n",
       "      <td>Rock</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>70</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>55</td>\n",
       "      <td>65</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Charizard</td>\n",
       "      <td>Fire</td>\n",
       "      <td>Flying</td>\n",
       "      <td>78</td>\n",
       "      <td>84</td>\n",
       "      <td>78</td>\n",
       "      <td>109</td>\n",
       "      <td>85</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>224</td>\n",
       "      <td>Octillery</td>\n",
       "      <td>Water</td>\n",
       "      <td>None</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>105</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>661</th>\n",
       "      <td>600</td>\n",
       "      <td>Klang</td>\n",
       "      <td>Steel</td>\n",
       "      <td>None</td>\n",
       "      <td>60</td>\n",
       "      <td>80</td>\n",
       "      <td>95</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>265</td>\n",
       "      <td>Wurmple</td>\n",
       "      <td>Bug</td>\n",
       "      <td>None</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>471</td>\n",
       "      <td>Glaceon</td>\n",
       "      <td>Ice</td>\n",
       "      <td>None</td>\n",
       "      <td>65</td>\n",
       "      <td>60</td>\n",
       "      <td>110</td>\n",
       "      <td>130</td>\n",
       "      <td>95</td>\n",
       "      <td>65</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>225</td>\n",
       "      <td>Delibird</td>\n",
       "      <td>Ice</td>\n",
       "      <td>Flying</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>720</td>\n",
       "      <td>HoopaHoopa Confined</td>\n",
       "      <td>Psychic</td>\n",
       "      <td>Ghost</td>\n",
       "      <td>80</td>\n",
       "      <td>110</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>109</td>\n",
       "      <td>Koffing</td>\n",
       "      <td>Poison</td>\n",
       "      <td>None</td>\n",
       "      <td>40</td>\n",
       "      <td>65</td>\n",
       "      <td>95</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>373</td>\n",
       "      <td>SalamenceMega Salamence</td>\n",
       "      <td>Dragon</td>\n",
       "      <td>Flying</td>\n",
       "      <td>95</td>\n",
       "      <td>145</td>\n",
       "      <td>130</td>\n",
       "      <td>120</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #                     Name   Type 1   Type 2  HP  Attack  Defense  \\\n",
       "370  338                  Solrock     Rock  Psychic  70      95       85   \n",
       "6      6                Charizard     Fire   Flying  78      84       78   \n",
       "242  224                Octillery    Water     None  75     105       75   \n",
       "661  600                    Klang    Steel     None  60      80       95   \n",
       "288  265                  Wurmple      Bug     None  45      45       35   \n",
       "..   ...                      ...      ...      ...  ..     ...      ...   \n",
       "522  471                  Glaceon      Ice     None  65      60      110   \n",
       "243  225                 Delibird      Ice   Flying  45      55       45   \n",
       "797  720      HoopaHoopa Confined  Psychic    Ghost  80     110       60   \n",
       "117  109                  Koffing   Poison     None  40      65       95   \n",
       "409  373  SalamenceMega Salamence   Dragon   Flying  95     145      130   \n",
       "\n",
       "     Sp. Atk  Sp. Def  Speed  Generation  Legendary  \n",
       "370       55       65     70           3      False  \n",
       "6        109       85    100           1      False  \n",
       "242      105       75     45           2      False  \n",
       "661       70       85     50           5      False  \n",
       "288       20       30     20           3      False  \n",
       "..       ...      ...    ...         ...        ...  \n",
       "522      130       95     65           4      False  \n",
       "243       65       45     75           2      False  \n",
       "797      150      130     70           6       True  \n",
       "117       60       45     35           1      False  \n",
       "409      120       90    120           3      False  \n",
       "\n",
       "[400 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "fifty_fifty_split_size = int(pokeaman.shape[0]*0.5)\n",
    "\n",
    "# Replace \"NaN\" (in the \"Type 2\" column with \"None\")\n",
    "pokeaman.fillna('None', inplace=True)\n",
    "\n",
    "np.random.seed(130)\n",
    "pokeaman_train,pokeaman_test = \\\n",
    "  train_test_split(pokeaman, train_size=fifty_fifty_split_size)\n",
    "pokeaman_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1230f",
   "metadata": {},
   "source": [
    "This code snippet prepares a dataset by handling missing values, setting a split size for a 50-50 train-test division, ensuring reproducibility, and splitting the dataset into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98ac8e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:10:47</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:10:47     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:10:47   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_spec3 = smf.ols(formula='HP ~ Attack + Defense', \n",
    "                      data=pokeaman_train)\n",
    "model3_fit = model_spec3.fit()\n",
    "model3_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ce4eb2",
   "metadata": {},
   "source": [
    "This summary gives insights into how well Attack and Defense predict HP and the statistical significance of each variable in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b773db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.14771558304519894\n",
      "'Out of sample' R-squared: 0.21208501873920738\n"
     ]
    }
   ],
   "source": [
    "yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model3_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model3)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b476b05d",
   "metadata": {},
   "source": [
    "The in-sample R-squared tells us how well the model fits the training data, while the out-of-sample R-squared shows how accurately it predicts HP in unseen test data. Comparing these values helps evaluate whether the model might be overfitting to the training set or generalizing well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8275414",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.467</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.764</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.23e-21</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:10:51</td>     <th>  Log-Likelihood:    </th> <td> -1738.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3603.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   337</td>      <th>  BIC:               </th> <td>   3855.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    62</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td>  521.5715</td> <td>  130.273</td> <td>    4.004</td> <td> 0.000</td> <td>  265.322</td> <td>  777.821</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>                                                <td>   -6.1179</td> <td>    2.846</td> <td>   -2.150</td> <td> 0.032</td> <td>  -11.716</td> <td>   -0.520</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                                           <td>   -8.1938</td> <td>    2.329</td> <td>   -3.518</td> <td> 0.000</td> <td>  -12.775</td> <td>   -3.612</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]</th>                                         <td>-1224.9610</td> <td>  545.105</td> <td>   -2.247</td> <td> 0.025</td> <td>-2297.199</td> <td> -152.723</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                                                          <td>   -6.1989</td> <td>    2.174</td> <td>   -2.851</td> <td> 0.005</td> <td>  -10.475</td> <td>   -1.923</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]</th>                                        <td> -102.4030</td> <td>   96.565</td> <td>   -1.060</td> <td> 0.290</td> <td> -292.350</td> <td>   87.544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense</th>                                                   <td>    0.0985</td> <td>    0.033</td> <td>    2.982</td> <td> 0.003</td> <td>    0.034</td> <td>    0.164</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]</th>                                 <td>   14.6361</td> <td>    6.267</td> <td>    2.336</td> <td> 0.020</td> <td>    2.310</td> <td>   26.963</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                                            <td>   -7.2261</td> <td>    2.178</td> <td>   -3.318</td> <td> 0.001</td> <td>  -11.511</td> <td>   -2.942</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]</th>                                          <td>  704.8798</td> <td>  337.855</td> <td>    2.086</td> <td> 0.038</td> <td>   40.309</td> <td> 1369.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                                                     <td>    0.1264</td> <td>    0.038</td> <td>    3.351</td> <td> 0.001</td> <td>    0.052</td> <td>    0.201</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]</th>                                   <td>    5.8648</td> <td>    2.692</td> <td>    2.179</td> <td> 0.030</td> <td>    0.570</td> <td>   11.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed</th>                                                    <td>    0.1026</td> <td>    0.039</td> <td>    2.634</td> <td> 0.009</td> <td>    0.026</td> <td>    0.179</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]</th>                                  <td>   -6.9266</td> <td>    3.465</td> <td>   -1.999</td> <td> 0.046</td> <td>  -13.742</td> <td>   -0.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed</th>                                             <td>   -0.0016</td> <td>    0.001</td> <td>   -2.837</td> <td> 0.005</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]</th>                           <td>   -0.0743</td> <td>    0.030</td> <td>   -2.477</td> <td> 0.014</td> <td>   -0.133</td> <td>   -0.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                                                     <td>   -5.3982</td> <td>    1.938</td> <td>   -2.785</td> <td> 0.006</td> <td>   -9.211</td> <td>   -1.586</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\")</th>                                   <td> -282.2496</td> <td>  126.835</td> <td>   -2.225</td> <td> 0.027</td> <td> -531.738</td> <td>  -32.761</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                                              <td>    0.1094</td> <td>    0.034</td> <td>    3.233</td> <td> 0.001</td> <td>    0.043</td> <td>    0.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\")</th>                            <td>   12.6503</td> <td>    5.851</td> <td>    2.162</td> <td> 0.031</td> <td>    1.141</td> <td>   24.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\")</th>                                             <td>    0.0628</td> <td>    0.028</td> <td>    2.247</td> <td> 0.025</td> <td>    0.008</td> <td>    0.118</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                           <td>    3.3949</td> <td>    1.783</td> <td>    1.904</td> <td> 0.058</td> <td>   -0.112</td> <td>    6.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\")</th>                                      <td>   -0.0012</td> <td>    0.000</td> <td>   -2.730</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")</th>                    <td>   -0.1456</td> <td>    0.065</td> <td>   -2.253</td> <td> 0.025</td> <td>   -0.273</td> <td>   -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                                               <td>    0.0624</td> <td>    0.031</td> <td>    2.027</td> <td> 0.043</td> <td>    0.002</td> <td>    0.123</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                             <td>   -3.2219</td> <td>    1.983</td> <td>   -1.625</td> <td> 0.105</td> <td>   -7.122</td> <td>    0.678</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>                                        <td>   -0.0014</td> <td>    0.001</td> <td>   -2.732</td> <td> 0.007</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                      <td>   -0.0695</td> <td>    0.033</td> <td>   -2.100</td> <td> 0.036</td> <td>   -0.135</td> <td>   -0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\")</th>                                       <td>   -0.0008</td> <td>    0.000</td> <td>   -1.743</td> <td> 0.082</td> <td>   -0.002</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>                     <td>    0.0334</td> <td>    0.021</td> <td>    1.569</td> <td> 0.117</td> <td>   -0.008</td> <td>    0.075</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\")</th>                                <td> 1.629e-05</td> <td> 6.92e-06</td> <td>    2.355</td> <td> 0.019</td> <td> 2.68e-06</td> <td> 2.99e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")</th>              <td>    0.0008</td> <td>    0.000</td> <td>    2.433</td> <td> 0.015</td> <td>    0.000</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                                                     <td>   -8.3636</td> <td>    2.346</td> <td>   -3.565</td> <td> 0.000</td> <td>  -12.978</td> <td>   -3.749</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Atk\")</th>                                   <td>  850.5436</td> <td>  385.064</td> <td>    2.209</td> <td> 0.028</td> <td>   93.112</td> <td> 1607.975</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                                              <td>    0.1388</td> <td>    0.040</td> <td>    3.500</td> <td> 0.001</td> <td>    0.061</td> <td>    0.217</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Atk\")</th>                            <td>    2.1809</td> <td>    1.136</td> <td>    1.920</td> <td> 0.056</td> <td>   -0.054</td> <td>    4.416</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Atk\")</th>                                             <td>    0.0831</td> <td>    0.038</td> <td>    2.162</td> <td> 0.031</td> <td>    0.007</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                           <td>   -7.3121</td> <td>    3.376</td> <td>   -2.166</td> <td> 0.031</td> <td>  -13.953</td> <td>   -0.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Atk\")</th>                                      <td>   -0.0014</td> <td>    0.001</td> <td>   -2.480</td> <td> 0.014</td> <td>   -0.003</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")</th>                    <td>   -0.0434</td> <td>    0.022</td> <td>   -2.010</td> <td> 0.045</td> <td>   -0.086</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                                               <td>    0.1011</td> <td>    0.035</td> <td>    2.872</td> <td> 0.004</td> <td>    0.032</td> <td>    0.170</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                             <td>  -12.6343</td> <td>    5.613</td> <td>   -2.251</td> <td> 0.025</td> <td>  -23.674</td> <td>   -1.594</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>                                        <td>   -0.0018</td> <td>    0.001</td> <td>   -3.102</td> <td> 0.002</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                      <td>    0.0151</td> <td>    0.009</td> <td>    1.609</td> <td> 0.109</td> <td>   -0.003</td> <td>    0.034</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Atk\")</th>                                       <td>   -0.0012</td> <td>    0.001</td> <td>   -1.860</td> <td> 0.064</td> <td>   -0.002</td> <td> 6.62e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>                     <td>    0.1210</td> <td>    0.054</td> <td>    2.260</td> <td> 0.024</td> <td>    0.016</td> <td>    0.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Atk\")</th>                                <td> 2.125e-05</td> <td>  9.1e-06</td> <td>    2.334</td> <td> 0.020</td> <td> 3.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")</th>              <td> 6.438e-06</td> <td> 7.69e-05</td> <td>    0.084</td> <td> 0.933</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                        <td>    0.1265</td> <td>    0.033</td> <td>    3.821</td> <td> 0.000</td> <td>    0.061</td> <td>    0.192</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                      <td>   -5.0544</td> <td>    2.506</td> <td>   -2.017</td> <td> 0.044</td> <td>   -9.983</td> <td>   -0.126</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                 <td>   -0.0021</td> <td>    0.001</td> <td>   -3.606</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>               <td>   -0.0346</td> <td>    0.017</td> <td>   -1.992</td> <td> 0.047</td> <td>   -0.069</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                <td>   -0.0012</td> <td>    0.000</td> <td>   -2.406</td> <td> 0.017</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0446</td> <td>    0.025</td> <td>    1.794</td> <td> 0.074</td> <td>   -0.004</td> <td>    0.093</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                         <td> 1.973e-05</td> <td> 7.28e-06</td> <td>    2.710</td> <td> 0.007</td> <td> 5.41e-06</td> <td>  3.4e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>    0.0005</td> <td>    0.000</td> <td>    1.957</td> <td> 0.051</td> <td>-2.56e-06</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                                  <td>   -0.0013</td> <td>    0.000</td> <td>   -2.740</td> <td> 0.006</td> <td>   -0.002</td> <td>   -0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                <td>    0.0841</td> <td>    0.040</td> <td>    2.125</td> <td> 0.034</td> <td>    0.006</td> <td>    0.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                           <td> 2.379e-05</td> <td> 7.85e-06</td> <td>    3.030</td> <td> 0.003</td> <td> 8.34e-06</td> <td> 3.92e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>         <td> 2.864e-05</td> <td> 7.73e-05</td> <td>    0.370</td> <td> 0.711</td> <td>   -0.000</td> <td>    0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                          <td> 1.284e-05</td> <td> 7.46e-06</td> <td>    1.721</td> <td> 0.086</td> <td>-1.83e-06</td> <td> 2.75e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0008</td> <td>    0.000</td> <td>   -2.085</td> <td> 0.038</td> <td>   -0.002</td> <td>-4.68e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>                   <td> -2.53e-07</td> <td>  1.1e-07</td> <td>   -2.292</td> <td> 0.023</td> <td> -4.7e-07</td> <td>-3.59e-08</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>-1.425e-06</td> <td> 1.14e-06</td> <td>   -1.249</td> <td> 0.212</td> <td>-3.67e-06</td> <td> 8.19e-07</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 1.2e+16. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                                                   &        HP        & \\textbf{  R-squared:         } &     0.467   \\\\\n",
       "\\textbf{Model:}                                                           &       OLS        & \\textbf{  Adj. R-squared:    } &     0.369   \\\\\n",
       "\\textbf{Method:}                                                          &  Least Squares   & \\textbf{  F-statistic:       } &     4.764   \\\\\n",
       "\\textbf{Date:}                                                            & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.23e-21   \\\\\n",
       "\\textbf{Time:}                                                            &     21:10:51     & \\textbf{  Log-Likelihood:    } &   -1738.6   \\\\\n",
       "\\textbf{No. Observations:}                                                &         400      & \\textbf{  AIC:               } &     3603.   \\\\\n",
       "\\textbf{Df Residuals:}                                                    &         337      & \\textbf{  BIC:               } &     3855.   \\\\\n",
       "\\textbf{Df Model:}                                                        &          62      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                                                 &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                                                        &     521.5715  &      130.273     &     4.004  &         0.000        &      265.322    &      777.821     \\\\\n",
       "\\textbf{Legendary[T.True]}                                                &      -6.1179  &        2.846     &    -2.150  &         0.032        &      -11.716    &       -0.520     \\\\\n",
       "\\textbf{Attack}                                                           &      -8.1938  &        2.329     &    -3.518  &         0.000        &      -12.775    &       -3.612     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]}                                         &   -1224.9610  &      545.105     &    -2.247  &         0.025        &    -2297.199    &     -152.723     \\\\\n",
       "\\textbf{Defense}                                                          &      -6.1989  &        2.174     &    -2.851  &         0.005        &      -10.475    &       -1.923     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]}                                        &    -102.4030  &       96.565     &    -1.060  &         0.290        &     -292.350    &       87.544     \\\\\n",
       "\\textbf{Attack:Defense}                                                   &       0.0985  &        0.033     &     2.982  &         0.003        &        0.034    &        0.164     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]}                                 &      14.6361  &        6.267     &     2.336  &         0.020        &        2.310    &       26.963     \\\\\n",
       "\\textbf{Speed}                                                            &      -7.2261  &        2.178     &    -3.318  &         0.001        &      -11.511    &       -2.942     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]}                                          &     704.8798  &      337.855     &     2.086  &         0.038        &       40.309    &     1369.450     \\\\\n",
       "\\textbf{Attack:Speed}                                                     &       0.1264  &        0.038     &     3.351  &         0.001        &        0.052    &        0.201     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]}                                   &       5.8648  &        2.692     &     2.179  &         0.030        &        0.570    &       11.160     \\\\\n",
       "\\textbf{Defense:Speed}                                                    &       0.1026  &        0.039     &     2.634  &         0.009        &        0.026    &        0.179     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]}                                  &      -6.9266  &        3.465     &    -1.999  &         0.046        &      -13.742    &       -0.111     \\\\\n",
       "\\textbf{Attack:Defense:Speed}                                             &      -0.0016  &        0.001     &    -2.837  &         0.005        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]}                           &      -0.0743  &        0.030     &    -2.477  &         0.014        &       -0.133    &       -0.015     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                                                     &      -5.3982  &        1.938     &    -2.785  &         0.006        &       -9.211    &       -1.586     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\")}                                   &    -282.2496  &      126.835     &    -2.225  &         0.027        &     -531.738    &      -32.761     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                                              &       0.1094  &        0.034     &     3.233  &         0.001        &        0.043    &        0.176     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\")}                            &      12.6503  &        5.851     &     2.162  &         0.031        &        1.141    &       24.160     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\")}                                             &       0.0628  &        0.028     &     2.247  &         0.025        &        0.008    &        0.118     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\")}                           &       3.3949  &        1.783     &     1.904  &         0.058        &       -0.112    &        6.902     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\")}                                      &      -0.0012  &        0.000     &    -2.730  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")}                    &      -0.1456  &        0.065     &    -2.253  &         0.025        &       -0.273    &       -0.018     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                                               &       0.0624  &        0.031     &     2.027  &         0.043        &        0.002    &        0.123     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\")}                             &      -3.2219  &        1.983     &    -1.625  &         0.105        &       -7.122    &        0.678     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}                                        &      -0.0014  &        0.001     &    -2.732  &         0.007        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                      &      -0.0695  &        0.033     &    -2.100  &         0.036        &       -0.135    &       -0.004     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\")}                                       &      -0.0008  &        0.000     &    -1.743  &         0.082        &       -0.002    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}                     &       0.0334  &        0.021     &     1.569  &         0.117        &       -0.008    &        0.075     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\")}                                &    1.629e-05  &     6.92e-06     &     2.355  &         0.019        &     2.68e-06    &     2.99e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")}              &       0.0008  &        0.000     &     2.433  &         0.015        &        0.000    &        0.001     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                                                     &      -8.3636  &        2.346     &    -3.565  &         0.000        &      -12.978    &       -3.749     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Atk\")}                                   &     850.5436  &      385.064     &     2.209  &         0.028        &       93.112    &     1607.975     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                                              &       0.1388  &        0.040     &     3.500  &         0.001        &        0.061    &        0.217     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Atk\")}                            &       2.1809  &        1.136     &     1.920  &         0.056        &       -0.054    &        4.416     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Atk\")}                                             &       0.0831  &        0.038     &     2.162  &         0.031        &        0.007    &        0.159     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                           &      -7.3121  &        3.376     &    -2.166  &         0.031        &      -13.953    &       -0.671     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Atk\")}                                      &      -0.0014  &        0.001     &    -2.480  &         0.014        &       -0.003    &       -0.000     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")}                    &      -0.0434  &        0.022     &    -2.010  &         0.045        &       -0.086    &       -0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                                               &       0.1011  &        0.035     &     2.872  &         0.004        &        0.032    &        0.170     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                             &     -12.6343  &        5.613     &    -2.251  &         0.025        &      -23.674    &       -1.594     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}                                        &      -0.0018  &        0.001     &    -3.102  &         0.002        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                      &       0.0151  &        0.009     &     1.609  &         0.109        &       -0.003    &        0.034     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Atk\")}                                       &      -0.0012  &        0.001     &    -1.860  &         0.064        &       -0.002    &     6.62e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}                     &       0.1210  &        0.054     &     2.260  &         0.024        &        0.016    &        0.226     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Atk\")}                                &    2.125e-05  &      9.1e-06     &     2.334  &         0.020        &     3.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")}              &    6.438e-06  &     7.69e-05     &     0.084  &         0.933        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                        &       0.1265  &        0.033     &     3.821  &         0.000        &        0.061    &        0.192     \\\\\n",
       "\\textbf{Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                      &      -5.0544  &        2.506     &    -2.017  &         0.044        &       -9.983    &       -0.126     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                 &      -0.0021  &        0.001     &    -3.606  &         0.000        &       -0.003    &       -0.001     \\\\\n",
       "\\textbf{Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}               &      -0.0346  &        0.017     &    -1.992  &         0.047        &       -0.069    &       -0.000     \\\\\n",
       "\\textbf{Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                &      -0.0012  &        0.000     &    -2.406  &         0.017        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0446  &        0.025     &     1.794  &         0.074        &       -0.004    &        0.093     \\\\\n",
       "\\textbf{Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                         &    1.973e-05  &     7.28e-06     &     2.710  &         0.007        &     5.41e-06    &      3.4e-05     \\\\\n",
       "\\textbf{Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &       0.0005  &        0.000     &     1.957  &         0.051        &    -2.56e-06    &        0.001     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                                  &      -0.0013  &        0.000     &    -2.740  &         0.006        &       -0.002    &       -0.000     \\\\\n",
       "\\textbf{Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                &       0.0841  &        0.040     &     2.125  &         0.034        &        0.006    &        0.162     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                           &    2.379e-05  &     7.85e-06     &     3.030  &         0.003        &     8.34e-06    &     3.92e-05     \\\\\n",
       "\\textbf{Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}         &    2.864e-05  &     7.73e-05     &     0.370  &         0.711        &       -0.000    &        0.000     \\\\\n",
       "\\textbf{Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                          &    1.284e-05  &     7.46e-06     &     1.721  &         0.086        &    -1.83e-06    &     2.75e-05     \\\\\n",
       "\\textbf{Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0008  &        0.000     &    -2.085  &         0.038        &       -0.002    &    -4.68e-05     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}                   &    -2.53e-07  &      1.1e-07     &    -2.292  &         0.023        &     -4.7e-07    &    -3.59e-08     \\\\\n",
       "\\textbf{Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &   -1.425e-06  &     1.14e-06     &    -1.249  &         0.212        &    -3.67e-06    &     8.19e-07     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 1.2e+16. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.467\n",
       "Model:                            OLS   Adj. R-squared:                  0.369\n",
       "Method:                 Least Squares   F-statistic:                     4.764\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           4.23e-21\n",
       "Time:                        21:10:51   Log-Likelihood:                -1738.6\n",
       "No. Observations:                 400   AIC:                             3603.\n",
       "Df Residuals:                     337   BIC:                             3855.\n",
       "Df Model:                          62                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                          521.5715    130.273      4.004      0.000     265.322     777.821\n",
       "Legendary[T.True]                                                   -6.1179      2.846     -2.150      0.032     -11.716      -0.520\n",
       "Attack                                                              -8.1938      2.329     -3.518      0.000     -12.775      -3.612\n",
       "Attack:Legendary[T.True]                                         -1224.9610    545.105     -2.247      0.025   -2297.199    -152.723\n",
       "Defense                                                             -6.1989      2.174     -2.851      0.005     -10.475      -1.923\n",
       "Defense:Legendary[T.True]                                         -102.4030     96.565     -1.060      0.290    -292.350      87.544\n",
       "Attack:Defense                                                       0.0985      0.033      2.982      0.003       0.034       0.164\n",
       "Attack:Defense:Legendary[T.True]                                    14.6361      6.267      2.336      0.020       2.310      26.963\n",
       "Speed                                                               -7.2261      2.178     -3.318      0.001     -11.511      -2.942\n",
       "Speed:Legendary[T.True]                                            704.8798    337.855      2.086      0.038      40.309    1369.450\n",
       "Attack:Speed                                                         0.1264      0.038      3.351      0.001       0.052       0.201\n",
       "Attack:Speed:Legendary[T.True]                                       5.8648      2.692      2.179      0.030       0.570      11.160\n",
       "Defense:Speed                                                        0.1026      0.039      2.634      0.009       0.026       0.179\n",
       "Defense:Speed:Legendary[T.True]                                     -6.9266      3.465     -1.999      0.046     -13.742      -0.111\n",
       "Attack:Defense:Speed                                                -0.0016      0.001     -2.837      0.005      -0.003      -0.001\n",
       "Attack:Defense:Speed:Legendary[T.True]                              -0.0743      0.030     -2.477      0.014      -0.133      -0.015\n",
       "Q(\"Sp. Def\")                                                        -5.3982      1.938     -2.785      0.006      -9.211      -1.586\n",
       "Legendary[T.True]:Q(\"Sp. Def\")                                    -282.2496    126.835     -2.225      0.027    -531.738     -32.761\n",
       "Attack:Q(\"Sp. Def\")                                                  0.1094      0.034      3.233      0.001       0.043       0.176\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\")                               12.6503      5.851      2.162      0.031       1.141      24.160\n",
       "Defense:Q(\"Sp. Def\")                                                 0.0628      0.028      2.247      0.025       0.008       0.118\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\")                               3.3949      1.783      1.904      0.058      -0.112       6.902\n",
       "Attack:Defense:Q(\"Sp. Def\")                                         -0.0012      0.000     -2.730      0.007      -0.002      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\")                       -0.1456      0.065     -2.253      0.025      -0.273      -0.018\n",
       "Speed:Q(\"Sp. Def\")                                                   0.0624      0.031      2.027      0.043       0.002       0.123\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\")                                -3.2219      1.983     -1.625      0.105      -7.122       0.678\n",
       "Attack:Speed:Q(\"Sp. Def\")                                           -0.0014      0.001     -2.732      0.007      -0.002      -0.000\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         -0.0695      0.033     -2.100      0.036      -0.135      -0.004\n",
       "Defense:Speed:Q(\"Sp. Def\")                                          -0.0008      0.000     -1.743      0.082      -0.002       0.000\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                         0.0334      0.021      1.569      0.117      -0.008       0.075\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\")                                 1.629e-05   6.92e-06      2.355      0.019    2.68e-06    2.99e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\")                  0.0008      0.000      2.433      0.015       0.000       0.001\n",
       "Q(\"Sp. Atk\")                                                        -8.3636      2.346     -3.565      0.000     -12.978      -3.749\n",
       "Legendary[T.True]:Q(\"Sp. Atk\")                                     850.5436    385.064      2.209      0.028      93.112    1607.975\n",
       "Attack:Q(\"Sp. Atk\")                                                  0.1388      0.040      3.500      0.001       0.061       0.217\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Atk\")                                2.1809      1.136      1.920      0.056      -0.054       4.416\n",
       "Defense:Q(\"Sp. Atk\")                                                 0.0831      0.038      2.162      0.031       0.007       0.159\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Atk\")                              -7.3121      3.376     -2.166      0.031     -13.953      -0.671\n",
       "Attack:Defense:Q(\"Sp. Atk\")                                         -0.0014      0.001     -2.480      0.014      -0.003      -0.000\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Atk\")                       -0.0434      0.022     -2.010      0.045      -0.086      -0.001\n",
       "Speed:Q(\"Sp. Atk\")                                                   0.1011      0.035      2.872      0.004       0.032       0.170\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Atk\")                               -12.6343      5.613     -2.251      0.025     -23.674      -1.594\n",
       "Attack:Speed:Q(\"Sp. Atk\")                                           -0.0018      0.001     -3.102      0.002      -0.003      -0.001\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                          0.0151      0.009      1.609      0.109      -0.003       0.034\n",
       "Defense:Speed:Q(\"Sp. Atk\")                                          -0.0012      0.001     -1.860      0.064      -0.002    6.62e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")                         0.1210      0.054      2.260      0.024       0.016       0.226\n",
       "Attack:Defense:Speed:Q(\"Sp. Atk\")                                 2.125e-05    9.1e-06      2.334      0.020    3.34e-06    3.92e-05\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Atk\")               6.438e-06   7.69e-05      0.084      0.933      -0.000       0.000\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                            0.1265      0.033      3.821      0.000       0.061       0.192\n",
       "Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                         -5.0544      2.506     -2.017      0.044      -9.983      -0.126\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                    -0.0021      0.001     -3.606      0.000      -0.003      -0.001\n",
       "Attack:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  -0.0346      0.017     -1.992      0.047      -0.069      -0.000\n",
       "Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                   -0.0012      0.000     -2.406      0.017      -0.002      -0.000\n",
       "Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0446      0.025      1.794      0.074      -0.004       0.093\n",
       "Attack:Defense:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                          1.973e-05   7.28e-06      2.710      0.007    5.41e-06     3.4e-05\n",
       "Attack:Defense:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           0.0005      0.000      1.957      0.051   -2.56e-06       0.001\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                                     -0.0013      0.000     -2.740      0.006      -0.002      -0.000\n",
       "Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    0.0841      0.040      2.125      0.034       0.006       0.162\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                            2.379e-05   7.85e-06      3.030      0.003    8.34e-06    3.92e-05\n",
       "Attack:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          2.864e-05   7.73e-05      0.370      0.711      -0.000       0.000\n",
       "Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                           1.284e-05   7.46e-06      1.721      0.086   -1.83e-06    2.75e-05\n",
       "Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0008      0.000     -2.085      0.038      -0.002   -4.68e-05\n",
       "Attack:Defense:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")                    -2.53e-07    1.1e-07     -2.292      0.023    -4.7e-07   -3.59e-08\n",
       "Attack:Defense:Speed:Legendary[T.True]:Q(\"Sp. Def\"):Q(\"Sp. Atk\") -1.425e-06   1.14e-06     -1.249      0.212   -3.67e-06    8.19e-07\n",
       "==============================================================================\n",
       "Omnibus:                      214.307   Durbin-Watson:                   1.992\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             2354.664\n",
       "Skew:                           2.026   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.174   Cond. No.                     1.20e+16\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.2e+16. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form = 'HP ~ Attack * Defense * Speed * Legendary'\n",
    "model4_linear_form += ' * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "# DO NOT try adding '* C(Generation) * C(Q(\"Type 1\")) * C(Q(\"Type 2\"))'\n",
    "# That's 6*18*19 = 6*18*19 possible interaction combinations...\n",
    "# ...a huge number that will blow up your computer\n",
    "\n",
    "model4_spec = smf.ols(formula=model4_linear_form, data=pokeaman_train)\n",
    "model4_fit = model4_spec.fit()\n",
    "model4_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a2e0f1",
   "metadata": {},
   "source": [
    "This model aims to capture complex relationships between features and HP, but it wisely limits itself to avoid unnecessary complexity and computational strain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706c5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.46709442115833855\n",
      "'Out of sample' R-squared: 0.002485342598992873\n"
     ]
    }
   ],
   "source": [
    "yhat_model4 = model4_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model4_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model4)[0,1]**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b8d43",
   "metadata": {},
   "source": [
    "The in-sample R-squared shows the model’s fit on the training data, while the out-of-sample R-squared indicates its predictive accuracy on new data. Comparing the two helps identify potential overfitting if the in-sample R-squared is high but the out-of-sample R-squared is significantly lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a641e371",
   "metadata": {},
   "source": [
    "The results illustrate how adding complexity (more predictors and interactions) improves training fit but can harm out-of-sample performance if it leads to overfitting. model3, though simple, generalizes better, while model4 captures more training data details at the cost of generalization. Based on R-squared comparisons, model3 is the preferable model for real-world predictions, as it balances interpretability with generalization. model4 may appear better on training data but fails to perform on new data. Simplicity can often lead to more reliable results, as overly complex models might fit noise in the data rather than general patterns. Regularly evaluating models on both training and test data is essential to prevent overfitting.\n",
    "\n",
    "In summary, while model4 appears more powerful on the training data, model3 is more reliable and generalizable across datasets. This outcome suggests that adding interactions and complexity should be approached cautiously to avoid overfitting, especially in datasets where simpler models might capture the essential patterns sufficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45992ab9",
   "metadata": {},
   "source": [
    "ChatGPT Summary:\n",
    "\n",
    "Absolutely! Here’s a summary of our conversation:\n",
    "\n",
    "1. **Initial Code Review**: We examined five cells of code involving data preparation, model building, and model evaluation in a predictive modeling task using a `pokeaman` dataset. The workflow involved splitting the data, handling missing values, building two models (one simple, one complex), and evaluating their predictive performance.\n",
    "\n",
    "2. **Model Descriptions**:\n",
    "   - **Model 3**: A simple linear regression model predicting `HP` using only `Attack` and `Defense`. This model offered modest predictive power (in-sample R-squared: ~0.15) but generalized well to the test data (out-of-sample R-squared: ~0.21).\n",
    "   - **Model 4**: A more complex model that included multiple predictors and interaction terms. This model fit the training data well (in-sample R-squared: ~0.47) but performed poorly on test data (out-of-sample R-squared near zero), indicating severe overfitting.\n",
    "\n",
    "3. **Insights from Results**:\n",
    "   - **Generalization vs. Overfitting**: `model3` performed consistently between training and test sets, while `model4` showed a large drop in performance from in-sample to out-of-sample R-squared, highlighting that added complexity can lead to overfitting.\n",
    "   - **Model Selection**: `model3`, despite its simplicity, was deemed more reliable for predictions on new data, as it balanced predictive power with generalizability.\n",
    "\n",
    "4. **Takeaway**: This analysis demonstrated the importance of evaluating both simple and complex models to find the balance between fit and generalization. Regularly comparing in-sample and out-of-sample performance helps identify the risk of overfitting and guides the choice of models that work well on unseen data.\n",
    "\n",
    "Let me know if there’s anything more you’d like to explore!\n",
    "\n",
    "ChatGPT Chatlog link: https://chatgpt.com/share/672fc3a4-8110-8004-8a94-6e9acb0628fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35084d",
   "metadata": {},
   "source": [
    "## 6. How the model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog and why the so-called multicollinearity in this \"design matrix\" (observed in np.corrcoef(model4_spec.exog)) contribues to the lack of \"out of sample\" generalization of predictions from model4_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a2b3cdd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:10:58</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   42.5882</td> <td>    3.580</td> <td>   11.897</td> <td> 0.000</td> <td>   35.551</td> <td>   49.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>    <td>    0.2472</td> <td>    0.041</td> <td>    6.051</td> <td> 0.000</td> <td>    0.167</td> <td>    0.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>   <td>    0.1001</td> <td>    0.045</td> <td>    2.201</td> <td> 0.028</td> <td>    0.011</td> <td>    0.190</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    343.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}    &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}            &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}           &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}             & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}             &     21:10:58     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:} &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}     &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}         &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}  &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                   & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept} &      42.5882  &        3.580     &    11.897  &         0.000        &       35.551    &       49.626     \\\\\n",
       "\\textbf{Attack}    &       0.2472  &        0.041     &     6.051  &         0.000        &        0.167    &        0.327     \\\\\n",
       "\\textbf{Defense}   &       0.1001  &        0.045     &     2.201  &         0.028        &        0.011    &        0.190     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     343.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:10:58   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     42.5882      3.580     11.897      0.000      35.551      49.626\n",
       "Attack         0.2472      0.041      6.051      0.000       0.167       0.327\n",
       "Defense        0.1001      0.045      2.201      0.028       0.011       0.190\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         343.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 343.0 WITHOUT to centering and scaling\n",
    "model3_fit.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b034f2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.148</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   34.40</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>1.66e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:11:09</td>     <th>  Log-Likelihood:    </th> <td> -1832.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3671.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   3683.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "             <td></td>               <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>              <td>   69.3025</td> <td>    1.186</td> <td>   58.439</td> <td> 0.000</td> <td>   66.971</td> <td>   71.634</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Attack))</th>  <td>    8.1099</td> <td>    1.340</td> <td>    6.051</td> <td> 0.000</td> <td>    5.475</td> <td>   10.745</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>scale(center(Defense))</th> <td>    2.9496</td> <td>    1.340</td> <td>    2.201</td> <td> 0.028</td> <td>    0.315</td> <td>    5.585</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>284.299</td> <th>  Durbin-Watson:     </th> <td>   2.006</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5870.841</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.720</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>20.963</td>  <th>  Cond. No.          </th> <td>    1.66</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}         &        HP        & \\textbf{  R-squared:         } &     0.148   \\\\\n",
       "\\textbf{Model:}                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.143   \\\\\n",
       "\\textbf{Method:}                &  Least Squares   & \\textbf{  F-statistic:       } &     34.40   \\\\\n",
       "\\textbf{Date:}                  & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  1.66e-14   \\\\\n",
       "\\textbf{Time:}                  &     21:11:09     & \\textbf{  Log-Likelihood:    } &   -1832.6   \\\\\n",
       "\\textbf{No. Observations:}      &         400      & \\textbf{  AIC:               } &     3671.   \\\\\n",
       "\\textbf{Df Residuals:}          &         397      & \\textbf{  BIC:               } &     3683.   \\\\\n",
       "\\textbf{Df Model:}              &           2      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}              &      69.3025  &        1.186     &    58.439  &         0.000        &       66.971    &       71.634     \\\\\n",
       "\\textbf{scale(center(Attack))}  &       8.1099  &        1.340     &     6.051  &         0.000        &        5.475    &       10.745     \\\\\n",
       "\\textbf{scale(center(Defense))} &       2.9496  &        1.340     &     2.201  &         0.028        &        0.315    &        5.585     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 284.299 & \\textbf{  Durbin-Watson:     } &    2.006  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5870.841  \\\\\n",
       "\\textbf{Skew:}          &   2.720 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  20.963 & \\textbf{  Cond. No.          } &     1.66  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.148\n",
       "Model:                            OLS   Adj. R-squared:                  0.143\n",
       "Method:                 Least Squares   F-statistic:                     34.40\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           1.66e-14\n",
       "Time:                        21:11:09   Log-Likelihood:                -1832.6\n",
       "No. Observations:                 400   AIC:                             3671.\n",
       "Df Residuals:                     397   BIC:                             3683.\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================\n",
       "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------\n",
       "Intercept                 69.3025      1.186     58.439      0.000      66.971      71.634\n",
       "scale(center(Attack))      8.1099      1.340      6.051      0.000       5.475      10.745\n",
       "scale(center(Defense))     2.9496      1.340      2.201      0.028       0.315       5.585\n",
       "==============================================================================\n",
       "Omnibus:                      284.299   Durbin-Watson:                   2.006\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5870.841\n",
       "Skew:                           2.720   Prob(JB):                         0.00\n",
       "Kurtosis:                      20.963   Cond. No.                         1.66\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from patsy import center, scale\n",
    "\n",
    "model3_linear_form_center_scale = \\\n",
    "  'HP ~ scale(center(Attack)) + scale(center(Defense))' \n",
    "model_spec3_center_scale = smf.ols(formula=model3_linear_form_center_scale,\n",
    "                                   data=pokeaman_train)\n",
    "model3_center_scale_fit = model_spec3_center_scale.fit()\n",
    "model3_center_scale_fit.summary()\n",
    "# \"Cond. No.\" is NOW 1.66 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ac725c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.663</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.54e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.663  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.54e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Defense))'\n",
    "model4_linear_form_CS += ' * scale(center(Speed)) * Legendary' \n",
    "model4_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# Legendary is an indicator, so we don't center and scale that\n",
    "\n",
    "model4_CS_spec = smf.ols(formula=model4_linear_form_CS, data=pokeaman_train)\n",
    "model4_CS_fit = model4_CS_spec.fit()\n",
    "model4_CS_fit.summary().tables[-1]  # Cond. No. is 2,250,000,000,000,000\n",
    "\n",
    "# The condition number is still bad even after centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d707688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>214.307</td> <th>  Durbin-Watson:     </th> <td>   1.992</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2354.664</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.026</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>14.174</td>  <th>  Cond. No.          </th> <td>1.20e+16</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 214.307 & \\textbf{  Durbin-Watson:     } &    1.992  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 2354.664  \\\\\n",
       "\\textbf{Skew:}          &   2.026 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  14.174 & \\textbf{  Cond. No.          } & 1.20e+16  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just as the condition number was very bad to start with\n",
    "model4_fit.summary().tables[-1]  # Cond. No. is 12,000,000,000,000,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fd09c7",
   "metadata": {},
   "source": [
    "The model4_linear_form (linear form specification of model4) creates new predictor variables as the columns of the so-called \"design matrix\" model4_spec.exog (model4_spec.exog.shape) used to predict the outcome variable model4_spec.endog through its use of main effects and interaction terms. These terms define combinations of predictors that become individual columns in the design matrix, model4_spec.exog, which is used by the model to predict the outcome variable model4_spec.endog (the target variable HP).\n",
    "\n",
    "model4_linear_form uses * to create a large number of predictors, including main effects and interaction terms, expanding the design matrix model4_spec.exog. This expansion enables the model to capture complex relationships but also introduces multicollinearity, which harms its predictive performance on new data.\n",
    "\n",
    "Multicollinearity in the design matrix model4_spec.exog contributes to overfitting by making the model’s coefficients unstable and overly sensitive to the training data. This results in poor out-of-sample generalization, as seen in the drastic drop in R-squared on test data, because the coefficients derived from the training data do not generalize well to new observations. Reducing model complexity or using regularization techniques could potentially improve the model’s ability to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449cd95",
   "metadata": {},
   "source": [
    "ChatGPT Summary:\n",
    "\n",
    "Certainly! Here’s a summary of our conversation starting from when you asked about the `model4_linear_form`:\n",
    "\n",
    "1. **Model4 Linear Form and Predictor Variables**:\n",
    "   - You asked how the `model4_linear_form` creates new predictor variables in the \"design matrix\" (`model4_spec.exog`). The formula for `model4_linear_form` includes main effects and interaction terms between multiple predictors (e.g., `Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, and `Sp. Atk`). These interactions create new predictor variables that are expanded into columns in the design matrix, resulting in a large number of predictor variables (main effects and interactions).\n",
    "   - The design matrix (`model4_spec.exog`) is a large matrix of predictors used by the model to predict the target variable `HP`. This matrix grows exponentially as interaction terms are included in the formula.\n",
    "\n",
    "2. **Multicollinearity and Its Effect on Model Performance**:\n",
    "   - Multicollinearity arises when the predictors in the design matrix are highly correlated with each other. In the case of `model4`, many of the interaction terms are strongly correlated with one another and with the main effects. This leads to instability in the model’s estimated coefficients.\n",
    "   - Multicollinearity makes it difficult for the model to separate the unique contribution of each predictor, and as a result, the model is prone to overfitting. This overfitting leads to high in-sample R-squared values but poor performance on new, unseen data (low out-of-sample R-squared).\n",
    "\n",
    "3. **Condition Number**:\n",
    "   - The **condition number** is a measure of the stability of the design matrix. A high condition number (observed in `model4_spec.exog`) indicates severe multicollinearity and poor numerical stability. This makes the model's coefficients sensitive to small changes in the data, leading to unreliable predictions.\n",
    "   - Despite attempts at centering and scaling the predictors, the condition number remained very high in `model4`, suggesting that the underlying model complexity (due to numerous interactions) was still contributing to the instability.\n",
    "\n",
    "4. **Centering and Scaling**:\n",
    "   - You then asked about centering and scaling, which are techniques used to address multicollinearity. **Centering** subtracts the mean from each predictor, making it zero-centered. **Scaling** divides each centered predictor by its standard deviation, ensuring all predictors are on a similar scale.\n",
    "   - Centering and scaling help reduce multicollinearity to some extent, but in `model4`, the model's complexity (due to high-dimensional interactions) led to a large condition number, indicating that the collinearity problem wasn't fully resolved.\n",
    "\n",
    "5. **Impact on Generalization**:\n",
    "   - The multicollinearity in `model4_spec.exog` contributed to poor **out-of-sample generalization** because it made the model overfit the training data. The model captured patterns specific to the training set that did not generalize well to the test set, resulting in high in-sample R-squared but very low out-of-sample R-squared.\n",
    "   - Despite the model fitting well to the training data, it failed to predict accurately for new data, highlighting the lack of generalization due to instability in coefficient estimates caused by multicollinearity.\n",
    "\n",
    "6. **Conclusion**:\n",
    "   - The high multicollinearity in `model4` (due to many correlated predictors and interactions) contributed to overfitting, making the model's coefficients unstable and unable to generalize to new data. Centering and scaling improved the condition number slightly, but the complexity of the model with many interactions remained problematic.\n",
    "   - To improve generalization, strategies such as reducing model complexity, using regularization, or applying dimensionality reduction techniques could be considered.\n",
    "   \n",
    "ChatGPT Chatlog link: https://chatgpt.com/share/672fc3a4-8110-8004-8a94-6e9acb0628fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b187e32b",
   "metadata": {},
   "source": [
    "## 7. The rationale and principles by which model5_linear_form is extended and developed from model3_fit and model4_fit; model6_linear_form is extended and developed from model5_linear_form; and model7_linear_form is extended and developed from model6_linear_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f71a67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.392</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.313</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.948</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>9.48e-19</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:21:02</td>     <th>  Log-Likelihood:    </th> <td> -1765.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3624.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   353</td>      <th>  BIC:               </th> <td>   3812.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    46</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "               <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                  <td>   10.1046</td> <td>   14.957</td> <td>    0.676</td> <td> 0.500</td> <td>  -19.312</td> <td>   39.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Legendary[T.True]</th>          <td>   -3.2717</td> <td>    4.943</td> <td>   -0.662</td> <td> 0.508</td> <td>  -12.992</td> <td>    6.449</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.2]</th>         <td>    9.2938</td> <td>    4.015</td> <td>    2.315</td> <td> 0.021</td> <td>    1.398</td> <td>   17.189</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.3]</th>         <td>    2.3150</td> <td>    3.915</td> <td>    0.591</td> <td> 0.555</td> <td>   -5.385</td> <td>   10.015</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.4]</th>         <td>    4.8353</td> <td>    4.149</td> <td>    1.165</td> <td> 0.245</td> <td>   -3.325</td> <td>   12.995</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.5]</th>         <td>   11.4838</td> <td>    3.960</td> <td>    2.900</td> <td> 0.004</td> <td>    3.696</td> <td>   19.272</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Generation)[T.6]</th>         <td>    4.9206</td> <td>    4.746</td> <td>    1.037</td> <td> 0.300</td> <td>   -4.413</td> <td>   14.254</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dark]</th>     <td>   -1.4155</td> <td>    6.936</td> <td>   -0.204</td> <td> 0.838</td> <td>  -15.057</td> <td>   12.226</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Dragon]</th>   <td>    0.8509</td> <td>    6.900</td> <td>    0.123</td> <td> 0.902</td> <td>  -12.720</td> <td>   14.422</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Electric]</th> <td>   -6.3641</td> <td>    6.537</td> <td>   -0.974</td> <td> 0.331</td> <td>  -19.220</td> <td>    6.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fairy]</th>    <td>   -1.9486</td> <td>   10.124</td> <td>   -0.192</td> <td> 0.847</td> <td>  -21.859</td> <td>   17.962</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fighting]</th> <td>    7.0308</td> <td>    7.432</td> <td>    0.946</td> <td> 0.345</td> <td>   -7.586</td> <td>   21.648</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Fire]</th>     <td>    3.0779</td> <td>    6.677</td> <td>    0.461</td> <td> 0.645</td> <td>  -10.055</td> <td>   16.210</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Flying]</th>   <td>   -2.1231</td> <td>   22.322</td> <td>   -0.095</td> <td> 0.924</td> <td>  -46.025</td> <td>   41.779</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ghost]</th>    <td>    5.7343</td> <td>    8.488</td> <td>    0.676</td> <td> 0.500</td> <td>  -10.960</td> <td>   22.429</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Grass]</th>    <td>    3.3275</td> <td>    5.496</td> <td>    0.605</td> <td> 0.545</td> <td>   -7.481</td> <td>   14.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ground]</th>   <td>    9.5118</td> <td>    7.076</td> <td>    1.344</td> <td> 0.180</td> <td>   -4.404</td> <td>   23.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Ice]</th>      <td>   -0.9313</td> <td>    7.717</td> <td>   -0.121</td> <td> 0.904</td> <td>  -16.108</td> <td>   14.246</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Normal]</th>   <td>   18.4816</td> <td>    5.312</td> <td>    3.479</td> <td> 0.001</td> <td>    8.034</td> <td>   28.929</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Poison]</th>   <td>    8.3411</td> <td>    7.735</td> <td>    1.078</td> <td> 0.282</td> <td>   -6.871</td> <td>   23.554</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Psychic]</th>  <td>    1.8061</td> <td>    6.164</td> <td>    0.293</td> <td> 0.770</td> <td>  -10.317</td> <td>   13.930</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Rock]</th>     <td>   -3.8558</td> <td>    6.503</td> <td>   -0.593</td> <td> 0.554</td> <td>  -16.645</td> <td>    8.933</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Steel]</th>    <td>   -4.0053</td> <td>    8.044</td> <td>   -0.498</td> <td> 0.619</td> <td>  -19.826</td> <td>   11.816</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 1\"))[T.Water]</th>    <td>    9.7988</td> <td>    5.166</td> <td>    1.897</td> <td> 0.059</td> <td>   -0.361</td> <td>   19.959</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dark]</th>     <td>    5.8719</td> <td>   15.185</td> <td>    0.387</td> <td> 0.699</td> <td>  -23.993</td> <td>   35.737</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Dragon]</th>   <td>   13.2777</td> <td>   14.895</td> <td>    0.891</td> <td> 0.373</td> <td>  -16.016</td> <td>   42.571</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Electric]</th> <td>   14.3228</td> <td>   17.314</td> <td>    0.827</td> <td> 0.409</td> <td>  -19.728</td> <td>   48.374</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fairy]</th>    <td>    2.8426</td> <td>   14.268</td> <td>    0.199</td> <td> 0.842</td> <td>  -25.218</td> <td>   30.903</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fighting]</th> <td>    1.9741</td> <td>   14.089</td> <td>    0.140</td> <td> 0.889</td> <td>  -25.735</td> <td>   29.683</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Fire]</th>     <td>    0.2001</td> <td>   15.730</td> <td>    0.013</td> <td> 0.990</td> <td>  -30.736</td> <td>   31.136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Flying]</th>   <td>    6.7292</td> <td>   13.581</td> <td>    0.495</td> <td> 0.621</td> <td>  -19.980</td> <td>   33.438</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ghost]</th>    <td>  -10.9402</td> <td>   15.895</td> <td>   -0.688</td> <td> 0.492</td> <td>  -42.201</td> <td>   20.321</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Grass]</th>    <td>    2.5119</td> <td>   14.540</td> <td>    0.173</td> <td> 0.863</td> <td>  -26.084</td> <td>   31.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ground]</th>   <td>   13.6042</td> <td>   13.655</td> <td>    0.996</td> <td> 0.320</td> <td>  -13.250</td> <td>   40.459</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Ice]</th>      <td>   19.7950</td> <td>   15.068</td> <td>    1.314</td> <td> 0.190</td> <td>   -9.840</td> <td>   49.430</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.None]</th>     <td>    7.6068</td> <td>   13.162</td> <td>    0.578</td> <td> 0.564</td> <td>  -18.279</td> <td>   33.493</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Normal]</th>   <td>   17.3191</td> <td>   17.764</td> <td>    0.975</td> <td> 0.330</td> <td>  -17.618</td> <td>   52.256</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Poison]</th>   <td>    0.7770</td> <td>   14.575</td> <td>    0.053</td> <td> 0.958</td> <td>  -27.887</td> <td>   29.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Psychic]</th>  <td>    4.2480</td> <td>   14.174</td> <td>    0.300</td> <td> 0.765</td> <td>  -23.628</td> <td>   32.124</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Rock]</th>     <td>    6.8858</td> <td>   16.221</td> <td>    0.424</td> <td> 0.671</td> <td>  -25.017</td> <td>   38.788</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Steel]</th>    <td>  -11.9623</td> <td>   14.973</td> <td>   -0.799</td> <td> 0.425</td> <td>  -41.409</td> <td>   17.485</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Q(\"Type 2\"))[T.Water]</th>    <td>    5.8097</td> <td>   14.763</td> <td>    0.394</td> <td> 0.694</td> <td>  -23.225</td> <td>   34.845</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                     <td>    0.2508</td> <td>    0.051</td> <td>    4.940</td> <td> 0.000</td> <td>    0.151</td> <td>    0.351</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Defense</th>                    <td>   -0.0096</td> <td>    0.060</td> <td>   -0.160</td> <td> 0.873</td> <td>   -0.127</td> <td>    0.108</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                      <td>   -0.1538</td> <td>    0.051</td> <td>   -2.998</td> <td> 0.003</td> <td>   -0.255</td> <td>   -0.053</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>               <td>    0.3484</td> <td>    0.059</td> <td>    5.936</td> <td> 0.000</td> <td>    0.233</td> <td>    0.464</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>               <td>    0.1298</td> <td>    0.051</td> <td>    2.525</td> <td> 0.012</td> <td>    0.029</td> <td>    0.231</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>286.476</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>5187.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.807</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>19.725</td>  <th>  Cond. No.          </th> <td>9.21e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 9.21e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}             &        HP        & \\textbf{  R-squared:         } &     0.392   \\\\\n",
       "\\textbf{Model:}                     &       OLS        & \\textbf{  Adj. R-squared:    } &     0.313   \\\\\n",
       "\\textbf{Method:}                    &  Least Squares   & \\textbf{  F-statistic:       } &     4.948   \\\\\n",
       "\\textbf{Date:}                      & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  9.48e-19   \\\\\n",
       "\\textbf{Time:}                      &     21:21:02     & \\textbf{  Log-Likelihood:    } &   -1765.0   \\\\\n",
       "\\textbf{No. Observations:}          &         400      & \\textbf{  AIC:               } &     3624.   \\\\\n",
       "\\textbf{Df Residuals:}              &         353      & \\textbf{  BIC:               } &     3812.   \\\\\n",
       "\\textbf{Df Model:}                  &          46      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}           &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                    & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                  &      10.1046  &       14.957     &     0.676  &         0.500        &      -19.312    &       39.521     \\\\\n",
       "\\textbf{Legendary[T.True]}          &      -3.2717  &        4.943     &    -0.662  &         0.508        &      -12.992    &        6.449     \\\\\n",
       "\\textbf{C(Generation)[T.2]}         &       9.2938  &        4.015     &     2.315  &         0.021        &        1.398    &       17.189     \\\\\n",
       "\\textbf{C(Generation)[T.3]}         &       2.3150  &        3.915     &     0.591  &         0.555        &       -5.385    &       10.015     \\\\\n",
       "\\textbf{C(Generation)[T.4]}         &       4.8353  &        4.149     &     1.165  &         0.245        &       -3.325    &       12.995     \\\\\n",
       "\\textbf{C(Generation)[T.5]}         &      11.4838  &        3.960     &     2.900  &         0.004        &        3.696    &       19.272     \\\\\n",
       "\\textbf{C(Generation)[T.6]}         &       4.9206  &        4.746     &     1.037  &         0.300        &       -4.413    &       14.254     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dark]}     &      -1.4155  &        6.936     &    -0.204  &         0.838        &      -15.057    &       12.226     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Dragon]}   &       0.8509  &        6.900     &     0.123  &         0.902        &      -12.720    &       14.422     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Electric]} &      -6.3641  &        6.537     &    -0.974  &         0.331        &      -19.220    &        6.491     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fairy]}    &      -1.9486  &       10.124     &    -0.192  &         0.847        &      -21.859    &       17.962     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fighting]} &       7.0308  &        7.432     &     0.946  &         0.345        &       -7.586    &       21.648     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Fire]}     &       3.0779  &        6.677     &     0.461  &         0.645        &      -10.055    &       16.210     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Flying]}   &      -2.1231  &       22.322     &    -0.095  &         0.924        &      -46.025    &       41.779     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ghost]}    &       5.7343  &        8.488     &     0.676  &         0.500        &      -10.960    &       22.429     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Grass]}    &       3.3275  &        5.496     &     0.605  &         0.545        &       -7.481    &       14.136     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ground]}   &       9.5118  &        7.076     &     1.344  &         0.180        &       -4.404    &       23.428     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Ice]}      &      -0.9313  &        7.717     &    -0.121  &         0.904        &      -16.108    &       14.246     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Normal]}   &      18.4816  &        5.312     &     3.479  &         0.001        &        8.034    &       28.929     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Poison]}   &       8.3411  &        7.735     &     1.078  &         0.282        &       -6.871    &       23.554     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Psychic]}  &       1.8061  &        6.164     &     0.293  &         0.770        &      -10.317    &       13.930     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Rock]}     &      -3.8558  &        6.503     &    -0.593  &         0.554        &      -16.645    &        8.933     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Steel]}    &      -4.0053  &        8.044     &    -0.498  &         0.619        &      -19.826    &       11.816     \\\\\n",
       "\\textbf{C(Q(\"Type 1\"))[T.Water]}    &       9.7988  &        5.166     &     1.897  &         0.059        &       -0.361    &       19.959     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dark]}     &       5.8719  &       15.185     &     0.387  &         0.699        &      -23.993    &       35.737     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Dragon]}   &      13.2777  &       14.895     &     0.891  &         0.373        &      -16.016    &       42.571     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Electric]} &      14.3228  &       17.314     &     0.827  &         0.409        &      -19.728    &       48.374     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fairy]}    &       2.8426  &       14.268     &     0.199  &         0.842        &      -25.218    &       30.903     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fighting]} &       1.9741  &       14.089     &     0.140  &         0.889        &      -25.735    &       29.683     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Fire]}     &       0.2001  &       15.730     &     0.013  &         0.990        &      -30.736    &       31.136     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Flying]}   &       6.7292  &       13.581     &     0.495  &         0.621        &      -19.980    &       33.438     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ghost]}    &     -10.9402  &       15.895     &    -0.688  &         0.492        &      -42.201    &       20.321     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Grass]}    &       2.5119  &       14.540     &     0.173  &         0.863        &      -26.084    &       31.108     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ground]}   &      13.6042  &       13.655     &     0.996  &         0.320        &      -13.250    &       40.459     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Ice]}      &      19.7950  &       15.068     &     1.314  &         0.190        &       -9.840    &       49.430     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.None]}     &       7.6068  &       13.162     &     0.578  &         0.564        &      -18.279    &       33.493     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Normal]}   &      17.3191  &       17.764     &     0.975  &         0.330        &      -17.618    &       52.256     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Poison]}   &       0.7770  &       14.575     &     0.053  &         0.958        &      -27.887    &       29.441     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Psychic]}  &       4.2480  &       14.174     &     0.300  &         0.765        &      -23.628    &       32.124     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Rock]}     &       6.8858  &       16.221     &     0.424  &         0.671        &      -25.017    &       38.788     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Steel]}    &     -11.9623  &       14.973     &    -0.799  &         0.425        &      -41.409    &       17.485     \\\\\n",
       "\\textbf{C(Q(\"Type 2\"))[T.Water]}    &       5.8097  &       14.763     &     0.394  &         0.694        &      -23.225    &       34.845     \\\\\n",
       "\\textbf{Attack}                     &       0.2508  &        0.051     &     4.940  &         0.000        &        0.151    &        0.351     \\\\\n",
       "\\textbf{Defense}                    &      -0.0096  &        0.060     &    -0.160  &         0.873        &       -0.127    &        0.108     \\\\\n",
       "\\textbf{Speed}                      &      -0.1538  &        0.051     &    -2.998  &         0.003        &       -0.255    &       -0.053     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}               &       0.3484  &        0.059     &     5.936  &         0.000        &        0.233    &        0.464     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}               &       0.1298  &        0.051     &     2.525  &         0.012        &        0.029    &        0.231     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 286.476 & \\textbf{  Durbin-Watson:     } &    1.917  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 5187.327  \\\\\n",
       "\\textbf{Skew:}          &   2.807 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  19.725 & \\textbf{  Cond. No.          } & 9.21e+03  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 9.21e+03. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.392\n",
       "Model:                            OLS   Adj. R-squared:                  0.313\n",
       "Method:                 Least Squares   F-statistic:                     4.948\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           9.48e-19\n",
       "Time:                        21:21:02   Log-Likelihood:                -1765.0\n",
       "No. Observations:                 400   AIC:                             3624.\n",
       "Df Residuals:                     353   BIC:                             3812.\n",
       "Df Model:                          46                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================================\n",
       "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------\n",
       "Intercept                     10.1046     14.957      0.676      0.500     -19.312      39.521\n",
       "Legendary[T.True]             -3.2717      4.943     -0.662      0.508     -12.992       6.449\n",
       "C(Generation)[T.2]             9.2938      4.015      2.315      0.021       1.398      17.189\n",
       "C(Generation)[T.3]             2.3150      3.915      0.591      0.555      -5.385      10.015\n",
       "C(Generation)[T.4]             4.8353      4.149      1.165      0.245      -3.325      12.995\n",
       "C(Generation)[T.5]            11.4838      3.960      2.900      0.004       3.696      19.272\n",
       "C(Generation)[T.6]             4.9206      4.746      1.037      0.300      -4.413      14.254\n",
       "C(Q(\"Type 1\"))[T.Dark]        -1.4155      6.936     -0.204      0.838     -15.057      12.226\n",
       "C(Q(\"Type 1\"))[T.Dragon]       0.8509      6.900      0.123      0.902     -12.720      14.422\n",
       "C(Q(\"Type 1\"))[T.Electric]    -6.3641      6.537     -0.974      0.331     -19.220       6.491\n",
       "C(Q(\"Type 1\"))[T.Fairy]       -1.9486     10.124     -0.192      0.847     -21.859      17.962\n",
       "C(Q(\"Type 1\"))[T.Fighting]     7.0308      7.432      0.946      0.345      -7.586      21.648\n",
       "C(Q(\"Type 1\"))[T.Fire]         3.0779      6.677      0.461      0.645     -10.055      16.210\n",
       "C(Q(\"Type 1\"))[T.Flying]      -2.1231     22.322     -0.095      0.924     -46.025      41.779\n",
       "C(Q(\"Type 1\"))[T.Ghost]        5.7343      8.488      0.676      0.500     -10.960      22.429\n",
       "C(Q(\"Type 1\"))[T.Grass]        3.3275      5.496      0.605      0.545      -7.481      14.136\n",
       "C(Q(\"Type 1\"))[T.Ground]       9.5118      7.076      1.344      0.180      -4.404      23.428\n",
       "C(Q(\"Type 1\"))[T.Ice]         -0.9313      7.717     -0.121      0.904     -16.108      14.246\n",
       "C(Q(\"Type 1\"))[T.Normal]      18.4816      5.312      3.479      0.001       8.034      28.929\n",
       "C(Q(\"Type 1\"))[T.Poison]       8.3411      7.735      1.078      0.282      -6.871      23.554\n",
       "C(Q(\"Type 1\"))[T.Psychic]      1.8061      6.164      0.293      0.770     -10.317      13.930\n",
       "C(Q(\"Type 1\"))[T.Rock]        -3.8558      6.503     -0.593      0.554     -16.645       8.933\n",
       "C(Q(\"Type 1\"))[T.Steel]       -4.0053      8.044     -0.498      0.619     -19.826      11.816\n",
       "C(Q(\"Type 1\"))[T.Water]        9.7988      5.166      1.897      0.059      -0.361      19.959\n",
       "C(Q(\"Type 2\"))[T.Dark]         5.8719     15.185      0.387      0.699     -23.993      35.737\n",
       "C(Q(\"Type 2\"))[T.Dragon]      13.2777     14.895      0.891      0.373     -16.016      42.571\n",
       "C(Q(\"Type 2\"))[T.Electric]    14.3228     17.314      0.827      0.409     -19.728      48.374\n",
       "C(Q(\"Type 2\"))[T.Fairy]        2.8426     14.268      0.199      0.842     -25.218      30.903\n",
       "C(Q(\"Type 2\"))[T.Fighting]     1.9741     14.089      0.140      0.889     -25.735      29.683\n",
       "C(Q(\"Type 2\"))[T.Fire]         0.2001     15.730      0.013      0.990     -30.736      31.136\n",
       "C(Q(\"Type 2\"))[T.Flying]       6.7292     13.581      0.495      0.621     -19.980      33.438\n",
       "C(Q(\"Type 2\"))[T.Ghost]      -10.9402     15.895     -0.688      0.492     -42.201      20.321\n",
       "C(Q(\"Type 2\"))[T.Grass]        2.5119     14.540      0.173      0.863     -26.084      31.108\n",
       "C(Q(\"Type 2\"))[T.Ground]      13.6042     13.655      0.996      0.320     -13.250      40.459\n",
       "C(Q(\"Type 2\"))[T.Ice]         19.7950     15.068      1.314      0.190      -9.840      49.430\n",
       "C(Q(\"Type 2\"))[T.None]         7.6068     13.162      0.578      0.564     -18.279      33.493\n",
       "C(Q(\"Type 2\"))[T.Normal]      17.3191     17.764      0.975      0.330     -17.618      52.256\n",
       "C(Q(\"Type 2\"))[T.Poison]       0.7770     14.575      0.053      0.958     -27.887      29.441\n",
       "C(Q(\"Type 2\"))[T.Psychic]      4.2480     14.174      0.300      0.765     -23.628      32.124\n",
       "C(Q(\"Type 2\"))[T.Rock]         6.8858     16.221      0.424      0.671     -25.017      38.788\n",
       "C(Q(\"Type 2\"))[T.Steel]      -11.9623     14.973     -0.799      0.425     -41.409      17.485\n",
       "C(Q(\"Type 2\"))[T.Water]        5.8097     14.763      0.394      0.694     -23.225      34.845\n",
       "Attack                         0.2508      0.051      4.940      0.000       0.151       0.351\n",
       "Defense                       -0.0096      0.060     -0.160      0.873      -0.127       0.108\n",
       "Speed                         -0.1538      0.051     -2.998      0.003      -0.255      -0.053\n",
       "Q(\"Sp. Def\")                   0.3484      0.059      5.936      0.000       0.233       0.464\n",
       "Q(\"Sp. Atk\")                   0.1298      0.051      2.525      0.012       0.029       0.231\n",
       "==============================================================================\n",
       "Omnibus:                      286.476   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5187.327\n",
       "Skew:                           2.807   Prob(JB):                         0.00\n",
       "Kurtosis:                      19.725   Cond. No.                     9.21e+03\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 9.21e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model5_linear_form = 'HP ~ Attack + Defense + Speed + Legendary'\n",
    "model5_linear_form += ' + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "model5_linear_form += ' + C(Generation) + C(Q(\"Type 1\")) + C(Q(\"Type 2\"))'\n",
    "\n",
    "model5_spec = smf.ols(formula=model5_linear_form, data=pokeaman_train)\n",
    "model5_fit = model5_spec.fit()\n",
    "model5_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45063447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3920134083531893\n",
      "'Out of sample' R-squared: 0.30015614488652215\n"
     ]
    }
   ],
   "source": [
    "yhat_model5 = model5_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model5_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model5)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b88f88b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.333</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.319</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   24.36</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>2.25e-30</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:21:14</td>     <th>  Log-Likelihood:    </th> <td> -1783.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3585.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   391</td>      <th>  BIC:               </th> <td>   3621.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                   <td></td>                     <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                          <td>   22.8587</td> <td>    3.876</td> <td>    5.897</td> <td> 0.000</td> <td>   15.238</td> <td>   30.479</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th> <td>   17.5594</td> <td>    3.339</td> <td>    5.258</td> <td> 0.000</td> <td>   10.994</td> <td>   24.125</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>  <td>    9.0301</td> <td>    3.172</td> <td>    2.847</td> <td> 0.005</td> <td>    2.794</td> <td>   15.266</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>         <td>    6.5293</td> <td>    2.949</td> <td>    2.214</td> <td> 0.027</td> <td>    0.732</td> <td>   12.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>         <td>    8.4406</td> <td>    2.711</td> <td>    3.114</td> <td> 0.002</td> <td>    3.112</td> <td>   13.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                             <td>    0.2454</td> <td>    0.037</td> <td>    6.639</td> <td> 0.000</td> <td>    0.173</td> <td>    0.318</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                              <td>   -0.1370</td> <td>    0.045</td> <td>   -3.028</td> <td> 0.003</td> <td>   -0.226</td> <td>   -0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                       <td>    0.3002</td> <td>    0.045</td> <td>    6.662</td> <td> 0.000</td> <td>    0.212</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                       <td>    0.1192</td> <td>    0.042</td> <td>    2.828</td> <td> 0.005</td> <td>    0.036</td> <td>    0.202</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>271.290</td> <th>  Durbin-Watson:     </th> <td>   1.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>4238.692</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.651</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>18.040</td>  <th>  Cond. No.          </th> <td>    618.</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                     &        HP        & \\textbf{  R-squared:         } &     0.333   \\\\\n",
       "\\textbf{Model:}                             &       OLS        & \\textbf{  Adj. R-squared:    } &     0.319   \\\\\n",
       "\\textbf{Method:}                            &  Least Squares   & \\textbf{  F-statistic:       } &     24.36   \\\\\n",
       "\\textbf{Date:}                              & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  2.25e-30   \\\\\n",
       "\\textbf{Time:}                              &     21:21:14     & \\textbf{  Log-Likelihood:    } &   -1783.6   \\\\\n",
       "\\textbf{No. Observations:}                  &         400      & \\textbf{  AIC:               } &     3585.   \\\\\n",
       "\\textbf{Df Residuals:}                      &         391      & \\textbf{  BIC:               } &     3621.   \\\\\n",
       "\\textbf{Df Model:}                          &           8      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                   &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                            & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                          &      22.8587  &        3.876     &     5.897  &         0.000        &       15.238    &       30.479     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]} &      17.5594  &        3.339     &     5.258  &         0.000        &       10.994    &       24.125     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}  &       9.0301  &        3.172     &     2.847  &         0.005        &        2.794    &       15.266     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}         &       6.5293  &        2.949     &     2.214  &         0.027        &        0.732    &       12.327     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}         &       8.4406  &        2.711     &     3.114  &         0.002        &        3.112    &       13.770     \\\\\n",
       "\\textbf{Attack}                             &       0.2454  &        0.037     &     6.639  &         0.000        &        0.173    &        0.318     \\\\\n",
       "\\textbf{Speed}                              &      -0.1370  &        0.045     &    -3.028  &         0.003        &       -0.226    &       -0.048     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                       &       0.3002  &        0.045     &     6.662  &         0.000        &        0.212    &        0.389     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                       &       0.1192  &        0.042     &     2.828  &         0.005        &        0.036    &        0.202     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 271.290 & \\textbf{  Durbin-Watson:     } &    1.999  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 4238.692  \\\\\n",
       "\\textbf{Skew:}          &   2.651 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  18.040 & \\textbf{  Cond. No.          } &     618.  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.333\n",
       "Model:                            OLS   Adj. R-squared:                  0.319\n",
       "Method:                 Least Squares   F-statistic:                     24.36\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           2.25e-30\n",
       "Time:                        21:21:14   Log-Likelihood:                -1783.6\n",
       "No. Observations:                 400   AIC:                             3585.\n",
       "Df Residuals:                     391   BIC:                             3621.\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "======================================================================================================\n",
       "                                         coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------\n",
       "Intercept                             22.8587      3.876      5.897      0.000      15.238      30.479\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]    17.5594      3.339      5.258      0.000      10.994      24.125\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]      9.0301      3.172      2.847      0.005       2.794      15.266\n",
       "I(Generation == 2)[T.True]             6.5293      2.949      2.214      0.027       0.732      12.327\n",
       "I(Generation == 5)[T.True]             8.4406      2.711      3.114      0.002       3.112      13.770\n",
       "Attack                                 0.2454      0.037      6.639      0.000       0.173       0.318\n",
       "Speed                                 -0.1370      0.045     -3.028      0.003      -0.226      -0.048\n",
       "Q(\"Sp. Def\")                           0.3002      0.045      6.662      0.000       0.212       0.389\n",
       "Q(\"Sp. Atk\")                           0.1192      0.042      2.828      0.005       0.036       0.202\n",
       "==============================================================================\n",
       "Omnibus:                      271.290   Durbin-Watson:                   1.999\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             4238.692\n",
       "Skew:                           2.651   Prob(JB):                         0.00\n",
       "Kurtosis:                      18.040   Cond. No.                         618.\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's something a little more reasonable...\n",
    "model6_linear_form = 'HP ~ Attack + Speed + Q(\"Sp. Def\") + Q(\"Sp. Atk\")'\n",
    "# And here we'll add the significant indicators from the previous model\n",
    "# https://chatgpt.com/share/81ab88df-4f07-49f9-a44a-de0cfd89c67c\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model6_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model6_linear_form += ' + I(Generation==2)'\n",
    "model6_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model6_spec = smf.ols(formula=model6_linear_form, data=pokeaman_train)\n",
    "model6_fit = model6_spec.fit()\n",
    "model6_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35ee1753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908\n",
      "'Out of sample' R-squared: 0.29572460427079933\n"
     ]
    }
   ],
   "source": [
    "yhat_model6 = model6_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bedebec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>HP</td>        <th>  R-squared:         </th> <td>   0.378</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.347</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   12.16</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 09 Nov 2024</td> <th>  Prob (F-statistic):</th> <td>4.20e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>21:21:25</td>     <th>  Log-Likelihood:    </th> <td> -1769.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   3579.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   380</td>      <th>  BIC:               </th> <td>   3659.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                     <td></td>                       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                              <td>   95.1698</td> <td>   34.781</td> <td>    2.736</td> <td> 0.007</td> <td>   26.783</td> <td>  163.556</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Normal\")[T.True]</th>     <td>   18.3653</td> <td>    3.373</td> <td>    5.445</td> <td> 0.000</td> <td>   11.733</td> <td>   24.997</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Q(\"Type 1\") == \"Water\")[T.True]</th>      <td>    9.2913</td> <td>    3.140</td> <td>    2.959</td> <td> 0.003</td> <td>    3.117</td> <td>   15.466</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 2)[T.True]</th>             <td>    7.0711</td> <td>    2.950</td> <td>    2.397</td> <td> 0.017</td> <td>    1.271</td> <td>   12.871</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>I(Generation == 5)[T.True]</th>             <td>    7.8557</td> <td>    2.687</td> <td>    2.923</td> <td> 0.004</td> <td>    2.572</td> <td>   13.140</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack</th>                                 <td>   -0.6975</td> <td>    0.458</td> <td>   -1.523</td> <td> 0.129</td> <td>   -1.598</td> <td>    0.203</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed</th>                                  <td>   -1.8147</td> <td>    0.554</td> <td>   -3.274</td> <td> 0.001</td> <td>   -2.905</td> <td>   -0.725</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed</th>                           <td>    0.0189</td> <td>    0.007</td> <td>    2.882</td> <td> 0.004</td> <td>    0.006</td> <td>    0.032</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\")</th>                           <td>   -0.5532</td> <td>    0.546</td> <td>   -1.013</td> <td> 0.312</td> <td>   -1.627</td> <td>    0.521</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\")</th>                    <td>    0.0090</td> <td>    0.007</td> <td>    1.311</td> <td> 0.191</td> <td>   -0.004</td> <td>    0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\")</th>                     <td>    0.0208</td> <td>    0.008</td> <td>    2.571</td> <td> 0.011</td> <td>    0.005</td> <td>    0.037</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\")</th>              <td>   -0.0002</td> <td> 9.06e-05</td> <td>   -2.277</td> <td> 0.023</td> <td>   -0.000</td> <td>-2.82e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Atk\")</th>                           <td>   -0.7277</td> <td>    0.506</td> <td>   -1.439</td> <td> 0.151</td> <td>   -1.722</td> <td>    0.267</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Atk\")</th>                    <td>    0.0136</td> <td>    0.005</td> <td>    2.682</td> <td> 0.008</td> <td>    0.004</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Atk\")</th>                     <td>    0.0146</td> <td>    0.007</td> <td>    2.139</td> <td> 0.033</td> <td>    0.001</td> <td>    0.028</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Atk\")</th>              <td>   -0.0002</td> <td>  5.4e-05</td> <td>   -3.383</td> <td> 0.001</td> <td>   -0.000</td> <td>-7.65e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>              <td>    0.0103</td> <td>    0.007</td> <td>    1.516</td> <td> 0.130</td> <td>   -0.003</td> <td>    0.024</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>       <td>   -0.0001</td> <td> 6.71e-05</td> <td>   -2.119</td> <td> 0.035</td> <td>   -0.000</td> <td>-1.03e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th>        <td>   -0.0002</td> <td> 8.82e-05</td> <td>   -2.075</td> <td> 0.039</td> <td>   -0.000</td> <td>-9.62e-06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")</th> <td>  2.03e-06</td> <td> 7.42e-07</td> <td>    2.734</td> <td> 0.007</td> <td>  5.7e-07</td> <td> 3.49e-06</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.34e+09. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                         &        HP        & \\textbf{  R-squared:         } &     0.378   \\\\\n",
       "\\textbf{Model:}                                 &       OLS        & \\textbf{  Adj. R-squared:    } &     0.347   \\\\\n",
       "\\textbf{Method:}                                &  Least Squares   & \\textbf{  F-statistic:       } &     12.16   \\\\\n",
       "\\textbf{Date:}                                  & Sat, 09 Nov 2024 & \\textbf{  Prob (F-statistic):} &  4.20e-29   \\\\\n",
       "\\textbf{Time:}                                  &     21:21:25     & \\textbf{  Log-Likelihood:    } &   -1769.5   \\\\\n",
       "\\textbf{No. Observations:}                      &         400      & \\textbf{  AIC:               } &     3579.   \\\\\n",
       "\\textbf{Df Residuals:}                          &         380      & \\textbf{  BIC:               } &     3659.   \\\\\n",
       "\\textbf{Df Model:}                              &          19      & \\textbf{                     } &             \\\\\n",
       "\\textbf{Covariance Type:}                       &    nonrobust     & \\textbf{                     } &             \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                                & \\textbf{coef} & \\textbf{std err} & \\textbf{t} & \\textbf{P$> |$t$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{Intercept}                              &      95.1698  &       34.781     &     2.736  &         0.007        &       26.783    &      163.556     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Normal\")[T.True]}     &      18.3653  &        3.373     &     5.445  &         0.000        &       11.733    &       24.997     \\\\\n",
       "\\textbf{I(Q(\"Type 1\") == \"Water\")[T.True]}      &       9.2913  &        3.140     &     2.959  &         0.003        &        3.117    &       15.466     \\\\\n",
       "\\textbf{I(Generation == 2)[T.True]}             &       7.0711  &        2.950     &     2.397  &         0.017        &        1.271    &       12.871     \\\\\n",
       "\\textbf{I(Generation == 5)[T.True]}             &       7.8557  &        2.687     &     2.923  &         0.004        &        2.572    &       13.140     \\\\\n",
       "\\textbf{Attack}                                 &      -0.6975  &        0.458     &    -1.523  &         0.129        &       -1.598    &        0.203     \\\\\n",
       "\\textbf{Speed}                                  &      -1.8147  &        0.554     &    -3.274  &         0.001        &       -2.905    &       -0.725     \\\\\n",
       "\\textbf{Attack:Speed}                           &       0.0189  &        0.007     &     2.882  &         0.004        &        0.006    &        0.032     \\\\\n",
       "\\textbf{Q(\"Sp. Def\")}                           &      -0.5532  &        0.546     &    -1.013  &         0.312        &       -1.627    &        0.521     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\")}                    &       0.0090  &        0.007     &     1.311  &         0.191        &       -0.004    &        0.023     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\")}                     &       0.0208  &        0.008     &     2.571  &         0.011        &        0.005    &        0.037     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\")}              &      -0.0002  &     9.06e-05     &    -2.277  &         0.023        &       -0.000    &    -2.82e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Atk\")}                           &      -0.7277  &        0.506     &    -1.439  &         0.151        &       -1.722    &        0.267     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Atk\")}                    &       0.0136  &        0.005     &     2.682  &         0.008        &        0.004    &        0.024     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Atk\")}                     &       0.0146  &        0.007     &     2.139  &         0.033        &        0.001    &        0.028     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Atk\")}              &      -0.0002  &      5.4e-05     &    -3.383  &         0.001        &       -0.000    &    -7.65e-05     \\\\\n",
       "\\textbf{Q(\"Sp. Def\"):Q(\"Sp. Atk\")}              &       0.0103  &        0.007     &     1.516  &         0.130        &       -0.003    &        0.024     \\\\\n",
       "\\textbf{Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}       &      -0.0001  &     6.71e-05     &    -2.119  &         0.035        &       -0.000    &    -1.03e-05     \\\\\n",
       "\\textbf{Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")}        &      -0.0002  &     8.82e-05     &    -2.075  &         0.039        &       -0.000    &    -9.62e-06     \\\\\n",
       "\\textbf{Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")} &     2.03e-06  &     7.42e-07     &     2.734  &         0.007        &      5.7e-07    &     3.49e-06     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lclc}\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{OLS Regression Results}\n",
       "\\end{center}\n",
       "\n",
       "Notes: \\newline\n",
       " [1] Standard Errors assume that the covariance matrix of the errors is correctly specified. \\newline\n",
       " [2] The condition number is large, 2.34e+09. This might indicate that there are \\newline\n",
       " strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                     HP   R-squared:                       0.378\n",
       "Model:                            OLS   Adj. R-squared:                  0.347\n",
       "Method:                 Least Squares   F-statistic:                     12.16\n",
       "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           4.20e-29\n",
       "Time:                        21:21:25   Log-Likelihood:                -1769.5\n",
       "No. Observations:                 400   AIC:                             3579.\n",
       "Df Residuals:                     380   BIC:                             3659.\n",
       "Df Model:                          19                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==========================================================================================================\n",
       "                                             coef    std err          t      P>|t|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------------------------\n",
       "Intercept                                 95.1698     34.781      2.736      0.007      26.783     163.556\n",
       "I(Q(\"Type 1\") == \"Normal\")[T.True]        18.3653      3.373      5.445      0.000      11.733      24.997\n",
       "I(Q(\"Type 1\") == \"Water\")[T.True]          9.2913      3.140      2.959      0.003       3.117      15.466\n",
       "I(Generation == 2)[T.True]                 7.0711      2.950      2.397      0.017       1.271      12.871\n",
       "I(Generation == 5)[T.True]                 7.8557      2.687      2.923      0.004       2.572      13.140\n",
       "Attack                                    -0.6975      0.458     -1.523      0.129      -1.598       0.203\n",
       "Speed                                     -1.8147      0.554     -3.274      0.001      -2.905      -0.725\n",
       "Attack:Speed                               0.0189      0.007      2.882      0.004       0.006       0.032\n",
       "Q(\"Sp. Def\")                              -0.5532      0.546     -1.013      0.312      -1.627       0.521\n",
       "Attack:Q(\"Sp. Def\")                        0.0090      0.007      1.311      0.191      -0.004       0.023\n",
       "Speed:Q(\"Sp. Def\")                         0.0208      0.008      2.571      0.011       0.005       0.037\n",
       "Attack:Speed:Q(\"Sp. Def\")                 -0.0002   9.06e-05     -2.277      0.023      -0.000   -2.82e-05\n",
       "Q(\"Sp. Atk\")                              -0.7277      0.506     -1.439      0.151      -1.722       0.267\n",
       "Attack:Q(\"Sp. Atk\")                        0.0136      0.005      2.682      0.008       0.004       0.024\n",
       "Speed:Q(\"Sp. Atk\")                         0.0146      0.007      2.139      0.033       0.001       0.028\n",
       "Attack:Speed:Q(\"Sp. Atk\")                 -0.0002    5.4e-05     -3.383      0.001      -0.000   -7.65e-05\n",
       "Q(\"Sp. Def\"):Q(\"Sp. Atk\")                  0.0103      0.007      1.516      0.130      -0.003       0.024\n",
       "Attack:Q(\"Sp. Def\"):Q(\"Sp. Atk\")          -0.0001   6.71e-05     -2.119      0.035      -0.000   -1.03e-05\n",
       "Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")           -0.0002   8.82e-05     -2.075      0.039      -0.000   -9.62e-06\n",
       "Attack:Speed:Q(\"Sp. Def\"):Q(\"Sp. Atk\")   2.03e-06   7.42e-07      2.734      0.007     5.7e-07    3.49e-06\n",
       "==============================================================================\n",
       "Omnibus:                      252.300   Durbin-Watson:                   1.953\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             3474.611\n",
       "Skew:                           2.438   Prob(JB):                         0.00\n",
       "Kurtosis:                      16.590   Cond. No.                     2.34e+09\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.34e+09. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhaps improve prediction...\n",
    "model7_linear_form = 'HP ~ Attack * Speed * Q(\"Sp. Def\") * Q(\"Sp. Atk\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form += ' + I(Generation==2)'\n",
    "model7_linear_form += ' + I(Generation==5)'\n",
    "\n",
    "model7_spec = smf.ols(formula=model7_linear_form, data=pokeaman_train)\n",
    "model7_fit = model7_spec.fit()\n",
    "model7_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b25f404d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456\n",
      "'Out of sample' R-squared: 0.35055389205977444\n"
     ]
    }
   ],
   "source": [
    "yhat_model7 = model7_fit.predict(pokeaman_test)\n",
    "y = pokeaman_test.HP\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared)\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a36b27c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>    15.4</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } &     15.4  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here's a slight change that seems to perhas improve prediction...\n",
    "model7_linear_form_CS = 'HP ~ scale(center(Attack)) * scale(center(Speed))'\n",
    "model7_linear_form_CS += ' * scale(center(Q(\"Sp. Def\"))) * scale(center(Q(\"Sp. Atk\")))'\n",
    "# We DO NOT center and scale indicator variables\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Normal\")'\n",
    "model7_linear_form_CS += ' + I(Q(\"Type 1\")==\"Water\")'\n",
    "model7_linear_form_CS += ' + I(Generation==2)'\n",
    "model7_linear_form_CS += ' + I(Generation==5)'\n",
    "\n",
    "model7_CS_spec = smf.ols(formula=model7_linear_form_CS, data=pokeaman_train)\n",
    "model7_CS_fit = model7_CS_spec.fit()\n",
    "model7_CS_fit.summary().tables[-1] \n",
    "# \"Cond. No.\" is NOW 15.4 due to centering and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d664427f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>252.300</td> <th>  Durbin-Watson:     </th> <td>   1.953</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>3474.611</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 2.438</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td>16.590</td>  <th>  Cond. No.          </th> <td>2.34e+09</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Omnibus:}       & 252.300 & \\textbf{  Durbin-Watson:     } &    1.953  \\\\\n",
       "\\textbf{Prob(Omnibus):} &   0.000 & \\textbf{  Jarque-Bera (JB):  } & 3474.611  \\\\\n",
       "\\textbf{Skew:}          &   2.438 & \\textbf{  Prob(JB):          } &     0.00  \\\\\n",
       "\\textbf{Kurtosis:}      &  16.590 & \\textbf{  Cond. No.          } & 2.34e+09  \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.table.SimpleTable'>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Cond. No.\" WAS 2,340,000,000 WITHOUT to centering and scaling\n",
    "model7_fit.summary().tables[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9815886c",
   "metadata": {},
   "source": [
    "The rationale and principles by which the models are extended and developed in this sequence are based on a combination of evidence-based hypothesis testing (i.e., examining the significance of individual predictors), in-sample and out-of-sample performance comparison, and model complexity management (balancing bias and variance).\n",
    "\n",
    "From model3 to model4: The rationale and principles by which the models are extended and developed in this sequence are based on a combination of evidence-based hypothesis testing (i.e., examining the significance of individual predictors), in-sample and out-of-sample performance comparison, and model complexity management (balancing bias and variance).\n",
    "\n",
    "From model3 and model4 to model5: The second extension adds categorical variables (Generation, Type 1, Type 2) to capture additional sources of variance in HP. This expansion aims to increase predictive accuracy by considering more factors but requires careful assessment of generalizability.\n",
    "\n",
    "From model5 to model6: model6 simplifies model5 by removing less informative or statistically insignificant variables. The rationale here is to improve generalization by reducing overfitting and making the model more parsimonious.\n",
    "\n",
    "From model6 to model7: The final model adds interaction terms and applies centering and scaling to address multicollinearity, aiming for a more robust model that can handle complex relationships between predictors without suffering from instability.\n",
    "\n",
    "The key principles are:\n",
    "\n",
    "- Each model is built by considering which predictors are likely to be significant based on domain knowledge and statistical significance testing. Non-significant predictors are removed in later models to avoid overfitting.\n",
    "\n",
    "- The performance of models is compared both on the training set (\"in-sample\") and test set (\"out-of-sample\") to ensure that the model generalizes well. Overfitting is assessed by checking for a large difference between in-sample and out-of-sample R-squared values.\n",
    "\n",
    "- As models become more complex (by adding predictors or interactions), the need to manage multicollinearity, stability, and interpretability becomes more important. This is addressed by centering, scaling, and simplifying the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625cadc0",
   "metadata": {},
   "source": [
    "ChatGPT Summary:\n",
    "\n",
    "Certainly! Here’s a summary of our conversation starting from when you introduced the code for **model5**:\n",
    "\n",
    "### **Overview of Model Building Process**:\n",
    "We discussed a series of model building exercises to predict Pokémon `HP` based on various features. The models were extended and developed iteratively, using **hypothesis testing**, **statistical significance**, and **out-of-sample performance** to guide the modeling decisions. Each step was aimed at improving the model's **predictive accuracy** and **generalizability**.\n",
    "\n",
    "### **Model5: Extending Model4 with More Predictors and Categorical Variables**:\n",
    "- **Model5 Linear Form**: The model was expanded by adding more variables (`Attack`, `Defense`, `Speed`, `Legendary`, `Sp. Def`, `Sp. Atk`) and categorical variables (`Generation`, `Type 1`, `Type 2`).\n",
    "- **Goal**: To improve the model by including more potential factors that could influence `HP`, aiming for more accurate predictions. The categorical variables help explain variations in `HP` due to Pokémon type and generation.\n",
    "- **Result**: This expansion helped capture a broader range of variability, but it required balancing the increased complexity with generalizability to avoid overfitting. The **in-sample** performance improved, but the **out-of-sample** (test set) performance was a key focus to ensure the model didn’t become too tailored to the training data.\n",
    "\n",
    "### **Model6: Simplifying Model5 Based on Significance**:\n",
    "- **Model6 Linear Form**: This model simplifies **Model5** by removing predictors that were deemed statistically insignificant (like certain `Type 1` categories) and keeping only the most meaningful variables.\n",
    "- **Goal**: To reduce overfitting by simplifying the model and improving its generalizability. The idea was to remove noise by focusing on the most significant predictors.\n",
    "- **Rationale**: Keeping only relevant predictors improves the model’s parsimony (simplicity) and ensures that the predictors included have a meaningful impact on the outcome variable (`HP`).\n",
    "\n",
    "### **Model7: Introducing Interactions and Centering/Scaling**:\n",
    "- **Model7 Linear Form**: The model was extended further by adding **interaction terms** between key predictors (`Attack`, `Speed`, `Sp. Def`, `Sp. Atk`). Centering and scaling were also applied to the continuous predictors to reduce multicollinearity and improve model stability.\n",
    "- **Goal**: By adding interaction terms, the model could better capture complex relationships between predictors. Centering and scaling were applied to mitigate potential issues with multicollinearity, which can cause instability in the model’s coefficients and predictions.\n",
    "- **Rationale**: This was an attempt to improve the model’s **out-of-sample** performance by addressing the potential multicollinearity and enhancing the model's ability to generalize.\n",
    "\n",
    "### **General Modeling Principles and Rationale**:\n",
    "- **Evidence-based Hypothesis Testing**: Throughout the process, the inclusion or exclusion of variables was guided by statistical significance and domain knowledge about what factors affect `HP`.\n",
    "- **Generalizability**: Models were assessed not only based on **in-sample** R-squared (training data performance) but also by how well they performed on **out-of-sample** (test data). If a model performed significantly better on the training data than the test data, it suggested that the model was overfitting.\n",
    "- **Complexity Management**: There was a continual effort to balance **model complexity** with the goal of improving predictive power. Simpler models (with fewer predictors) were tested alongside more complex ones to ensure that unnecessary complexity did not degrade model performance.\n",
    "- **Model Underfitting and Overfitting**: Underfitting was considered when a model did not utilize enough of the available predictors to make accurate predictions. Overfitting was considered when the model was too complex and failed to generalize well to new data. By extending and simplifying the models, the aim was to find a good balance where the model was complex enough to capture relevant patterns but simple enough to avoid overfitting.\n",
    "\n",
    "### **Key Takeaways**:\n",
    "1. **Model Development**: Each model was developed with the goal of increasing accuracy, while carefully considering the impact on generalizability to avoid overfitting.\n",
    "2. **Hypothesis Testing**: Statistical testing (e.g., p-values) guided the decision to add or remove predictors based on their relevance.\n",
    "3. **Multicollinearity**: Centering and scaling were introduced to address multicollinearity, ensuring that the model remained numerically stable and interpretable.\n",
    "4. **Performance Evaluation**: The models were evaluated using both **in-sample** (training data) and **out-of-sample** (test data) metrics, with a focus on achieving **better generalization**.\n",
    "\n",
    "In summary, the models were built incrementally, starting with simpler models and then adding complexity in a controlled way, guided by statistical testing and the goal of improving out-of-sample performance.\n",
    "\n",
    "ChatGPT Chatlog link: https://chatgpt.com/share/672fc3a4-8110-8004-8a94-6e9acb0628fd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58d84f2",
   "metadata": {},
   "source": [
    "## 8. Writing a for loop to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" model performance metric actualizations (by not using np.random.seed(130) within each loop iteration); the meaning of my results and purpose of this demonstration\n",
    "\n",
    "Original code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a834c8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px  # etc.\n",
    "\n",
    "songs_training_data,songs_testing_data = train_test_split(songs, train_size=31)\n",
    "linear_form = 'danceability ~ energy * loudness + energy * mode'\n",
    "   \n",
    "reps = 100\n",
    "in_sample_Rsquared = np.array([0.0]*reps)\n",
    "out_of_sample_Rsquared = np.array([0.0]*reps)\n",
    "for i in range(reps):\n",
    "    songs_training_data,songs_testing_data = \\\n",
    "      train_test_split(songs, train_size=31)\n",
    "    final_model_fit = smf.ols(formula=linear_form, \n",
    "                              data=songs_training_data).fit()\n",
    "    in_sample_Rsquared[i] = final_model_fit.rsquared\n",
    "    out_of_sample_Rsquared[i] = \\\n",
    "      np.corrcoef(songs_testing_data.danceability, \n",
    "                  final_model_fit.predict(songs_testing_data))[0,1]**2\n",
    "    \n",
    "df = pd.DataFrame({\"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "                   \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared})   >  \n",
    "fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    "                     y=\"Out of Sample Performance (Rsquared)\")\n",
    "fig.add_trace(go.Scatter(x=[0,1], y=[0,1], name=\"y=x\", line_shape='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933dceb2",
   "metadata": {},
   "source": [
    "For loop code ChatGPT helped create to create, collect, and visualize many different paired \"in sample\" and \"out of sample\" model performance metric actualizations (by not using np.random.seed(130) within each loop iteration). Slightly edited and repurposed code to match the 50-50 train-test split analysis and data in the train_test_split method of \"Question 5\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e33434a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'pokeaman' is already loaded as a DataFrame\n",
    "# For example:\n",
    "# pokeaman = pd.read_csv('pokeaman_data.csv')\n",
    "\n",
    "# Define the number of repetitions for the random train-test splits\n",
    "reps = 100\n",
    "\n",
    "# Initialize arrays to collect the R-squared values\n",
    "in_sample_Rsquared = np.zeros(reps)\n",
    "out_of_sample_Rsquared = np.zeros(reps)\n",
    "\n",
    "# Loop to repeat the process for multiple random splits\n",
    "for i in range(reps):\n",
    "    # Perform the train-test split (50-50 split)\n",
    "    pokeaman_train, pokeaman_test = train_test_split(pokeaman, train_size=0.5)\n",
    "    \n",
    "    # Fit the model to the training data (linear form specification)\n",
    "    model_spec3 = smf.ols(formula='HP ~ Attack + Defense', data=pokeaman_train)\n",
    "    model3_fit = model_spec3.fit()\n",
    "    \n",
    "    # Calculate 'in-sample' R-squared\n",
    "    in_sample_Rsquared[i] = model3_fit.rsquared\n",
    "    \n",
    "    # Predict 'out-of-sample' HP values and calculate 'out-of-sample' R-squared\n",
    "    yhat_model3 = model3_fit.predict(pokeaman_test)\n",
    "    y = pokeaman_test.HP\n",
    "    out_of_sample_Rsquared[i] = np.corrcoef(y, yhat_model3)[0, 1] ** 2\n",
    "\n",
    "# Create a DataFrame for visualization of the results\n",
    "df = pd.DataFrame({\n",
    "    \"In Sample Performance (Rsquared)\": in_sample_Rsquared,\n",
    "    \"Out of Sample Performance (Rsquared)\": out_of_sample_Rsquared\n",
    "})\n",
    "\n",
    "# Visualize the results using Plotly\n",
    "fig = px.scatter(df, x=\"In Sample Performance (Rsquared)\", \n",
    "                 y=\"Out of Sample Performance (Rsquared)\",\n",
    "                 title=\"In-Sample vs. Out-of-Sample R-squared\",\n",
    "                 labels={\"In Sample Performance (Rsquared)\": \"In-Sample R-squared\",\n",
    "                         \"Out of Sample Performance (Rsquared)\": \"Out-of-Sample R-squared\"})\n",
    "\n",
    "# Add a reference line (y = x) for better comparison\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], name=\"y=x\", line_shape='linear'))\n",
    "\n",
    "# Show the plot\n",
    "fig.show(renderer='png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d7b125",
   "metadata": {},
   "source": [
    "Meaning of my results:\n",
    "\n",
    "The code and its results focus on evaluating the generalizability and stability of a machine learning model through multiple train-test splits and examining the relationship between \"in-sample\" and \"out-of-sample\" performance metrics.\n",
    "\n",
    "For In-Sample R-squared, this value indicates how well the model fits the data it was trained on. R-squared is a statistical measure that represents the proportion of the variance in the dependent variable (in this case, HP) that is predictable from the independent variables (here, Attack and Defense). High in-sample R-squared means the model explains most of the variance in the training data.\n",
    "\n",
    "For Out-of-Sample R-squared, this value reflects how well the model generalizes to unseen data (the test set), and it is calculated by comparing the actual test set values to the predicted values. The out-of-sample R-squared is crucial for understanding the model's ability to make accurate predictions on data it has not seen before. A high out-of-sample R-squared means that the model generalizes well to new, unseen data.\n",
    "\n",
    "The Purpose of This Demonstration:\n",
    "\n",
    "- Assessing Overfitting vs. Underfitting:\n",
    "    - Overfitting occurs when a model is too complex and performs very well on the training data but poorly on the test data. This suggests the model is memorizing the training data rather than learning the underlying patterns, leading to poor generalization.\n",
    "    - Underfitting happens when the model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test sets.\n",
    "    - By running multiple random train-test splits (via the for loop), you can observe the stability of the model's performance. If the in-sample R-squared is significantly higher than the out-of-sample R-squared, this could indicate overfitting. Conversely, if both values are low, it might suggest underfitting.\n",
    "- Exploring Model Stability:\n",
    "    - By performing the train-test split multiple times, we assess the consistency of the model's performance. If the model performs well on different random subsets of data, it is likely a more stable model that can generalize well.\n",
    "    - Randomization (by not fixing the seed) ensures that we're not biased by any specific data split, allowing us to see how the model might behave in different scenarios and thus better gauge its robustness.\n",
    "    \n",
    "Interpreting the Results:\n",
    "- Scatter Plot of In-Sample vs. Out-of-Sample R-squared:\n",
    "    - The x-axis represents the in-sample R-squared values, which reflect how well the model fits the training data.\n",
    "    - The y-axis represents the out-of-sample R-squared values, indicating how well the model performs on unseen data.\n",
    "    - Points close to the y=x line (the diagonal) indicate that the model is performing similarly on both the training and test data, suggesting that the model is neither overfitting nor underfitting and is generalizing well.\n",
    "    - Points above the y=x line indicate situations where the model performs better on the test data than on the training data, which might happen when the model is too simple (underfitting).\n",
    "    - Points below the y=x line indicate situations where the model performs better on the training data than on the test data, which is a typical sign of overfitting.\n",
    "    \n",
    "Purpose of Using Multiple Splits:\n",
    "- Simulating Real-World Variability: In real-world scenarios, we rarely work with a single train-test split. The performance of models can vary depending on how the data is split. By running the model across many different splits, we simulate the real-world process of model validation, which gives a more reliable estimate of how the model might perform on future, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b901fe7",
   "metadata": {},
   "source": [
    "ChatGPT Summary:\n",
    "\n",
    "Sure! Here's a summary of our conversation:\n",
    "\n",
    "1. **Task Overview**: You wanted to write code to evaluate and visualize the performance of a linear regression model over multiple random train-test splits. The goal was to compare the \"in-sample\" (training) and \"out-of-sample\" (testing) R-squared values to assess the model’s stability and generalizability.\n",
    "\n",
    "2. **Code Breakdown**:\n",
    "   - You provided code that performs multiple train-test splits (without fixing the random seed), fits a linear regression model, and calculates both in-sample and out-of-sample R-squared values.\n",
    "   - We adapted this to work with your dataset (`pokeaman`), specifically using `Attack` and `Defense` to predict `HP` in the model, running the process 100 times to collect R-squared values.\n",
    "   - We visualized the results with a scatter plot comparing in-sample and out-of-sample performance, adding a reference line (`y=x`) to help interpret the results.\n",
    "\n",
    "3. **Purpose of the Analysis**:\n",
    "   - **In-Sample R-squared** measures how well the model fits the training data.\n",
    "   - **Out-of-Sample R-squared** measures how well the model generalizes to new, unseen data.\n",
    "   - The demonstration helps detect overfitting (high in-sample but low out-of-sample performance) or underfitting (low performance on both sets).\n",
    "   - Multiple train-test splits simulate real-world variability, providing a more reliable assessment of the model’s performance.\n",
    "\n",
    "4. **Key Insights**:\n",
    "   - By running the model on many different random splits, you assess both its **stability** (how consistent the performance is) and **generalizability** (how well it performs on unseen data).\n",
    "   - The scatter plot allows you to visualize how the model performs across different splits, and the reference line helps interpret whether the model is overfitting or underfitting.\n",
    "\n",
    "5. **Conclusion**: This process helps in understanding the trade-off between model bias (underfitting) and variance (overfitting) and guides further model tuning to improve generalizability and performance.\n",
    "\n",
    "This overall analysis is crucial for ensuring that the model can generalize well to new data and isn’t overly specialized to the training dataset.\n",
    "\n",
    "ChatGPT chatlog link: https://chatgpt.com/share/672fec9d-f2ec-8004-982e-22d99082b228"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa55195",
   "metadata": {},
   "source": [
    "## 9. Understanding the meaning of the illustration below; and explaining it in my own words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef3584f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.005496616883231457 (original)\n",
      "'In sample' R-squared:     0.5726118179916575 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.11151363354803218 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model7_gen1_predict_future_fit = model7_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model7_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b68bb335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.37818209127432456 (original)\n",
      "'Out of sample' R-squared: 0.005496616883231457 (original)\n",
      "'In sample' R-squared:     0.3904756578094535 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.23394915464343125 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model7_gen1to5_predict_future = smf.ols(formula=model7_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model7_gen1to5_predict_future_fit = model7_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model7_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model7)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model7_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model7_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25eba546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.0036021428831465347 (original)\n",
      "'In sample' R-squared:     0.4433880517727282 (gen1_predict_future)\n",
      "'Out of sample' R-squared: 0.1932858534276128 (gen1_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation==1])\n",
    "model6_gen1_predict_future_fit = model6_gen1_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1_predict_future_fit.rsquared, \"(gen1_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation!=1].HP\n",
    "yhat = model6_gen1_predict_future_fit.predict(pokeaman[pokeaman.Generation!=1])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1_predict_future)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a97e365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'In sample' R-squared:     0.3326310334310908 (original)\n",
      "'Out of sample' R-squared: 0.0036021428831465347 (original)\n",
      "'In sample' R-squared:     0.33517279824114776 (gen1to5_predict_future)\n",
      "'Out of sample' R-squared: 0.26262690178799936 (gen1to5_predict_future)\n"
     ]
    }
   ],
   "source": [
    "model6_gen1to5_predict_future = smf.ols(formula=model6_linear_form,\n",
    "                                   data=pokeaman[pokeaman.Generation!=6])\n",
    "model6_gen1to5_predict_future_fit = model6_gen1to5_predict_future.fit()\n",
    "print(\"'In sample' R-squared:    \", model6_fit.rsquared, \"(original)\")\n",
    "y = pokeaman_test.HP\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat_model6)[0,1]**2, \"(original)\")\n",
    "print(\"'In sample' R-squared:    \", model6_gen1to5_predict_future_fit.rsquared, \"(gen1to5_predict_future)\")\n",
    "y = pokeaman[pokeaman.Generation==6].HP\n",
    "yhat = model6_gen1to5_predict_future_fit.predict(pokeaman[pokeaman.Generation==6])\n",
    "print(\"'Out of sample' R-squared:\", np.corrcoef(y,yhat)[0,1]**2, \"(gen1to5_predict_future)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ffa75",
   "metadata": {},
   "source": [
    "The meaning of the illustration is:\n",
    "\n",
    "It helps to demonstrate the generalizability and out-of-sample performance of two models (model6 and model7) in the context of sequential data (e.g., Pokémon Generations). It compares how each model performs when predicting data from future \"Generations\" that it hasn't seen during training. This highlights key differences in how simpler models (like model6) and more complex models (like model7) handle new, unseen data.\n",
    "\n",
    "1. Model Performance on Different Generations: The output is comparing the performance of two models (model6 and model7) using different approaches:\n",
    "- Training the model on Generation 1 and predicting data from other generations.\n",
    "- Training the model on Generations 1 to 5 and predicting data from Generation 6 (a completely unseen generation).\n",
    "2. In-Sample vs. Out-of-Sample R-Squared:\n",
    "- In-sample R-squared: This measures how well the model fits the training data (i.e., how well the model explains the variance in the data it was trained on).\n",
    "- Out-of-sample R-squared: This measures the model’s ability to generalize to new, unseen data. It’s a key indicator of generalizability (i.e., how well the model performs when applied to data it hasn’t seen before).\n",
    "\n",
    "Breakdown of each illustration:\n",
    "\n",
    "1. Model 7: Training on Generation 1\n",
    "\n",
    "- Training on Generation 1 and testing on all other generations.\n",
    "\n",
    "- In-sample R-squared for model7:\n",
    "    - 'In sample' R-squared: 0.378 (original model) — shows that model7 fits the data reasonably well, explaining 37.8% of the variance in the training data (Generation 1).\n",
    "    \n",
    "- Out-of-sample R-squared for model7:\n",
    "    - 'Out of sample' R-squared: 0.0055 (original) — shows poor generalization to the test data, explaining only 0.55% of the variance in the out-of-sample data (the rest of the generations).\n",
    "\n",
    "- In-sample R-squared (using Gen 1 for training and testing):\n",
    "    - 'In sample' R-squared: 0.573 — When the model is tested on Generation 1 (the same data it was trained on), it fits better, explaining 57.3% of the variance.\n",
    "\n",
    "- Out-of-sample R-squared (predicting future generations from Gen 1):\n",
    "    - 'Out of sample' R-squared: 0.1115 — Still relatively low (11.15%) but better than the previous out-of-sample result. This suggests the model is somewhat able to predict future generations, but not very well.\n",
    "    \n",
    "2. Model 7: Training on Generations 1-5, Predicting on Generation 6\n",
    "\n",
    "- Training on Generations 1-5 and testing on Generation 6.\n",
    "\n",
    "- In-sample R-squared for model7:\n",
    "    - Same as before: 'In sample' R-squared: 0.378 (original model) — It fits the original training data reasonably well.\n",
    "\n",
    "- Out-of-sample R-squared for model7:\n",
    "    - 'Out of sample' R-squared: 0.0055 (original) — Same as before, indicating poor generalization on test data.\n",
    "\n",
    "- In-sample R-squared for the model trained on Generations 1 to 5:\n",
    "    - 'In sample' R-squared: 0.390 — A very slight improvement when training on data from multiple generations, but still a relatively weak fit.\n",
    "\n",
    "- Out-of-sample R-squared (on Generation 6):\n",
    "    - 'Out of sample' R-squared: 0.234 — This is better than the previous out-of-sample result, but still not very good. It suggests that the model doesn't perform well on Generation 6 data, even though it was trained on earlier generations.\n",
    "    \n",
    "3. Model 6: Training on Generation 1\n",
    "\n",
    "- Training on Generation 1 and testing on all other generations.\n",
    "\n",
    "- In-sample R-squared for model6:\n",
    "    - 'In sample' R-squared: 0.333 (original model) — A slightly lower fit than model7, with 33.3% of the variance explained.\n",
    "\n",
    "- Out-of-sample R-squared for model6:\n",
    "    - 'Out of sample' R-squared: 0.0036 (original) — Very poor generalization, explaining just 0.36% of the variance in the out-of-sample data.\n",
    "\n",
    "- In-sample R-squared (using Gen 1 for training and testing):\n",
    "    - 'In sample' R-squared: 0.443 — Better fit when using Generation 1 data for both training and testing.\n",
    "\n",
    "- Out-of-sample R-squared (predicting future generations from Gen 1):\n",
    "    - 'Out of sample' R-squared: 0.193 — This is still poor generalization, but slightly better than model7’s prediction on future generations.\n",
    "    \n",
    "4. Model 6: Training on Generations 1-5, Predicting on Generation 6\n",
    "\n",
    "- Training on Generations 1-5 and testing on Generation 6.\n",
    "\n",
    "- In-sample R-squared for model6:\n",
    "    - Same as before: 'In sample' R-squared: 0.333 (original model).\n",
    "\n",
    "- Out-of-sample R-squared for model6:\n",
    "    - 'Out of sample' R-squared: 0.0036 (original) — Very poor generalization on the test data.\n",
    "\n",
    "- In-sample R-squared (using Generations 1 to 5 for training):\n",
    "    - 'In sample' R-squared: 0.335 — A slight improvement when using more data (Generations 1 to 5).\n",
    "\n",
    "- Out-of-sample R-squared (on Generation 6):\n",
    "    - 'Out of sample' R-squared: 0.263 — Improved prediction on Generation 6 compared to using only Generation 1.\n",
    "    \n",
    "The key message is that model7 performs worse in out-of-sample tests (especially when predicting on future generations), likely because it is overfitting the training data. Its performance drops dramatically when predicting unseen generations, indicating poor generalization. On the other hand, model6 shows more consistent and stable generalization, even though its in-sample performance is lower.\n",
    "\n",
    "Despite having better in-sample performance, the complexity of model7 causes it to overfit and fail to generalize well. Model6, being simpler, is more stable and generalizes better to unseen data, especially when predicting future generations.\n",
    "\n",
    "This example reinforces the importance of generalizability when dealing with data that arrives sequentially over time. Predicting future generations requires a model that can generalize well to new, unseen data, rather than just fitting the training set perfectly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa6075c",
   "metadata": {},
   "source": [
    "End of HW 7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
